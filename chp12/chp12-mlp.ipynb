{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcdc3a1",
   "metadata": {},
   "source": [
    "# Implementing a Multilayer Artificial Neural Network from Sractch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0785f85",
   "metadata": {},
   "source": [
    "## Introducting the multiplayer neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b564a3",
   "metadata": {},
   "source": [
    "We are going to start our exploration of Neural networks with a **M**ulti**L**ayer **P**erceptron (MLP). We'll use it to learn how to connect multiple single neurons. The following picture illustrates an MLP consisting of **three** layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651aca6",
   "metadata": {},
   "source": [
    "![MLP illustration](./etc/mlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bff43",
   "metadata": {},
   "source": [
    "The above MLP has one:\n",
    "\n",
    "- Input Layer (1st layer)\n",
    "- Hidden Layer (2nd layer)\n",
    "- Output Layer (3rd layer)\n",
    "\n",
    "And notice, our MLP is ***fully connected***. It means that each unit in the input layer is **connected to all** units in the hidden layer, and each units in the hidden layer is **connected to all** units in the output layer. If the network had _more than one_ hidden layer, we would call it a **deep artificial Neural Network (NN)**. And the field of **Deep Learning** is concerned with the development algorithm to help us train such structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef7c15",
   "metadata": {},
   "source": [
    "Now, let's define how we will refer to elements in our neural network:\n",
    "\n",
    "- We denote the $i^{th}$ activation unit in the $l^{th}$ layer: $a_i^{(l)}$\n",
    "- We denote the connection between the $k^{th}$ unit in layer $l$ to the $j^{th}$ unit in layer $l + 1$ as: $w_{k,j}^{l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb847730",
   "metadata": {},
   "source": [
    "The book denotes the weight matrix that connects the input to the hidden layer as: **$W^{(h)}$** and the weight matrix that connects the hidden layer to the output layer as: **$W^{(out)}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e497a2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Wait, what **Weight matrix?!**\n",
    "\n",
    "Okay, let me walk you through how to derive $W^{(h)}$ and you can try to do the same for $W^{(out)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd774c3",
   "metadata": {},
   "source": [
    "Let's say we have a data set of only **ONE** training example, and we want to \"forward\" propagate that one input from the input layer (in) to the hidden layer (h). To do that, we have to compute $a_1^{(h)}$, $a_2^{(h)}$, $\\dots$, $a_d^{(h)}$. For instance,\n",
    "\n",
    "$a_1^{(h)} = \\phi(z_1^{(h)})$\n",
    "\n",
    "$z_1^{(h)} = a_0^{(in)} w_{0,1}^{(in)} + a_1^{(in)} w_{1,1}^{(in)} + \\dots + a_m^{(in)} w_{m,1}^{(in)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d68d68",
   "metadata": {},
   "source": [
    "Now consider the following matrix for $z^{(h)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7df8b6",
   "metadata": {},
   "source": [
    "![Weight Matrix demonstration](./etc/demo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da994c",
   "metadata": {},
   "source": [
    "Notice, the weight matrix $W^{(h)}$ is an $m x d$ matrix where $d$ is the number of units in the hidden layer and $m$ is the number of units in the input layer, including the bias unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b551a",
   "metadata": {},
   "source": [
    "As you can see, our training example is be multiplied by that weight matrix, to compute its net input vector $Z^{(h)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a41a44",
   "metadata": {},
   "source": [
    "$Z^{(h)} = a^{(in)}W^{(h)}$\n",
    "\n",
    "$a^{(h)} = \\phi(Z^{(h)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcc2f9",
   "metadata": {},
   "source": [
    "Here $a^{(in)}$, is our training example (an $1 \\times m$ matrix). And since $W^{(h)}$ is an $m \\times d$ matrix, the resulting net input vector $Z^{(h)}$ is an $1 \\times d$ row matrix. That net input vector is then passed to the activation function to compute $a^{(h)}$ which is $1 \\times d$ matrix. Now, we can generalize this computation to all $n$ example in the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d98fb4",
   "metadata": {},
   "source": [
    "$Z^{(h)} = A^{(in)}W^{(h)}$. Here, $A^{(in)}$ is an $n x m$ matrix.\n",
    "\n",
    "**Each** training example (row) in the matrix is multiplied by the weight matrix. This multiplication happens through the matrix-matrix multiplication of $A^{(in)}$ and $W^{(h)}$ because each row of $A^{(in)}$ gets multiplied to $W^{(h)}$. \n",
    "\n",
    "This matrix-matrix multiplication results in an $m x d$, net input matrix $Z^{(h)}$; where the $i^{(th)}$ row in $Z^{(h)}$ is the result of the matrix-vector multiplication of the $i^{(th)}$ training example and the weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ddc359",
   "metadata": {},
   "source": [
    "Finally, we apply the function $\\phi(\\bullet)$ to each value in the net input matrix to get the $n \\times d$ **activation** matrix. \n",
    "\n",
    "$A^{(h)} = \\phi(Z^{(h)})$\n",
    "\n",
    "The $i^{(th)}$ row in the activation matrix contains the values the activation units in the hidden layer will contains after the $i^{th}$ training example forward propagates :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33158a",
   "metadata": {},
   "source": [
    "Similarly, we can write the activation of the **output layer** in vectorized form for multiple examples:\n",
    "\n",
    "$Z^{(out)} = A^{(h)}W^{(out)}$ and $A^{(out)} = \\phi(Z^{(out)})$ where $A^{(out)}$ is an $n \\times t$ matrix.\n",
    "\n",
    "**Challenge**: Can you see why? Can you derive $W^{(out)}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62f666",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ok, cool. But why did we use those matrices?\n",
    "\n",
    "For code effeciency and readability. We used our basic linear algebra skills to write the computations in a more compact way, so we do not use computationally expensive Python `for` loops. It also helps us delegate those computations to a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793c65d",
   "metadata": {},
   "source": [
    "## Classifying handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e7a2d",
   "metadata": {},
   "source": [
    "Let's implement and train our first multilayer NN to classify handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a19075",
   "metadata": {},
   "source": [
    "### Obtaining and preparing the MINIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b47ebe",
   "metadata": {},
   "source": [
    "The dataset is freely available on Yann Lecun's website. It consists of the following four parts:\n",
    "\n",
    "1. Training dataset images\n",
    "2. Training dataset labels\n",
    "3. Test dataset images\n",
    "4. Test dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9074af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
