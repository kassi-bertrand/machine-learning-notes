{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcdc3a1",
   "metadata": {},
   "source": [
    "# Implementing a Multilayer Artificial Neural Network from Sractch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0785f85",
   "metadata": {},
   "source": [
    "## Introducting the multiplayer neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b564a3",
   "metadata": {},
   "source": [
    "We are going to start our exploration of Neural networks with a **M**ulti**L**ayer **P**erceptron (MLP). We'll use it to learn how to connect multiple single neurons. The following picture illustrates an MLP consisting of **three** layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aff359",
   "metadata": {},
   "source": [
    "![mlp](etc/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bff43",
   "metadata": {},
   "source": [
    "The above MLP has one:\n",
    "\n",
    "- Input Layer (1st layer)\n",
    "- Hidden Layer (2nd layer)\n",
    "- Output Layer (3rd layer)\n",
    "\n",
    "And notice, our MLP is **fully connected**. It means that each unit in the input layer is **connected to all** units in the hidden layer, and each units in the hidden layer is **connected to all** units in the output layer. If the network had _more than one_ hidden layer, we would call it a **deep artificial Neural Network (NN)**. And the field of **Deep Learning** is concerned with the development algorithm to help us train such structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef7c15",
   "metadata": {},
   "source": [
    "Now, let's define how we will refer to elements in our neural network:\n",
    "\n",
    "- We denote the $i^{th}$ activation unit in the $l^{th}$ layer: $a_i^{(l)}$\n",
    "- We denote the connection between the $k^{th}$ unit in layer $l - 1$ to the $j^{th}$ unit in layer $l$ as: $w_{k,j}^{l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb847730",
   "metadata": {},
   "source": [
    "The book denotes the weight matrix that connects the input to the hidden layer as: **$W^{(h)}$** and the weight matrix that connects the hidden layer to the output layer as: **$W^{(out)}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e497a2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Wait, what **Weight matrix?!**\n",
    "\n",
    "Okay, let me walk you through how to derive $W^{(h)}$ and you can try to do the same for $W^{(out)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd774c3",
   "metadata": {},
   "source": [
    "Let's say we have a data set of only **ONE** training example, and we want to \"forward\" propagate that one input from the input layer (in) to the hidden layer (h). To do that, we have to compute $a_1^{(h)}$, $a_2^{(h)}$, $\\dots$, $a_d^{(h)}$. For instance,\n",
    "\n",
    "$a_1^{(h)} = \\phi(z_1^{(h)})$\n",
    "\n",
    "$z_1^{(h)} = a_0^{(in)} w_{0,1}^{(h)} + a_1^{(in)} w_{1,1}^{(h)} + \\dots + a_m^{(in)} w_{m,1}^{(h)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d68d68",
   "metadata": {},
   "source": [
    "Now consider the following matrix for $z^{(h)}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa7df8b6",
   "metadata": {},
   "source": [
    "![demo-2.jpg](etc/demo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da994c",
   "metadata": {},
   "source": [
    "Notice, the weight matrix $W^{(h)}$ is an $m \\times d$ matrix where $d$ is the number of units in the hidden layer and $m$ is the number of units in the input layer, including the bias unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b551a",
   "metadata": {},
   "source": [
    "As you can see, our training example is being multiplied by that weight matrix, to compute its net input vector $Z^{(h)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a41a44",
   "metadata": {},
   "source": [
    "$Z^{(h)} = a^{(in)}W^{(h)}$\n",
    "\n",
    "$a^{(h)} = \\phi(Z^{(h)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcc2f9",
   "metadata": {},
   "source": [
    "Here $a^{(in)}$, is our training example (an $1 \\times m$ matrix). And since $W^{(h)}$ is an $m \\times d$ matrix, the resulting net input vector $Z^{(h)}$ is an $1 \\times d$ row matrix. That net input vector is then passed to the activation function to compute $a^{(h)}$ which is $1 \\times d$ matrix. Now, we can generalize this computation to all $n$ example in the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d98fb4",
   "metadata": {},
   "source": [
    "$Z^{(h)} = A^{(in)}W^{(h)}$. Here, $A^{(in)}$ is an $n \\times m$ matrix.\n",
    "\n",
    "**Each** training example (row) in the matrix is multiplied by the weight matrix. This multiplication happens through the matrix-matrix multiplication of $A^{(in)}$ and $W^{(h)}$ because each row of $A^{(in)}$ gets multiplied to $W^{(h)}$. \n",
    "\n",
    "This matrix-matrix multiplication results in an $n \\times d$, net input matrix $Z^{(h)}$; where the $i^{(th)}$ row in $Z^{(h)}$ is the result of the matrix-vector multiplication of the $i^{(th)}$ training example and the weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ddc359",
   "metadata": {},
   "source": [
    "Finally, we apply the function $\\phi(\\bullet)$ to each value in the net input matrix to get the $n \\times d$ **activation** matrix. \n",
    "\n",
    "$A^{(h)} = \\phi(Z^{(h)})$\n",
    "\n",
    "The $i^{(th)}$ row in the activation matrix contains the values the activation units in the hidden layer will contains after the $i^{th}$ training example forward propagates :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e33158a",
   "metadata": {},
   "source": [
    "Similarly, we can write the activation of the **output layer** in vectorized form for multiple examples:\n",
    "\n",
    "$Z^{(out)} = A^{(h)}W^{(out)}$ and $A^{(out)} = \\phi(Z^{(out)})$ where $A^{(out)}$ is an $n \\times t$ matrix.\n",
    "\n",
    "**Challenge**: Can you derive $W^{(out)}$ on your own? Here is mine, in case you find it difficult. As you can see on the following image, the $W^{(out)}$ is an $d \\times t$ matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0895aab",
   "metadata": {},
   "source": [
    "![demo W_out](etc/demo-w_out.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62f666",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ok, cool. But why did we use those matrices?\n",
    "\n",
    "For code effeciency and readability. We used our basic linear algebra skills to write the computations in a more compact way, so we do not use computationally expensive Python `for` loops. It also helps us delegate those computations to a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793c65d",
   "metadata": {},
   "source": [
    "## Classifying handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e7a2d",
   "metadata": {},
   "source": [
    "Let's implement and train our first multilayer NN to classify handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a19075",
   "metadata": {},
   "source": [
    "### Obtaining and preparing the MINIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b47ebe",
   "metadata": {},
   "source": [
    "The dataset is freely available on [Yann Lecun's website](http://yann.lecun.com/exdb/mnist/). It consists of the following four parts:\n",
    "\n",
    "1. **Training dataset images**: train-images-idx3-ubyte.gz (60,000 examples)\n",
    "2. **Training dataset labels**: train-labels-idx1-ubyte.gz (60,000 labels)\n",
    "3. **Test dataset images**: t10k-images-idx3-ubyte.gz (10,000 examples)\n",
    "4. **Test dataset labels**: t10k-labels-idx1-ubyte.gz (10,000 labels)\n",
    "\n",
    "Each part is in its own archive file, and must be unzipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f102c",
   "metadata": {},
   "source": [
    "With the dataset unzipped, we need to read the images and the labels into NumPy arrays. Both are stored as _binary_ files. We acheive this using the `load_mnist` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a7630",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\billy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/billy/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST dataset from 'path' \"\"\"\n",
    "    \n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' %kind) #Notice, it is a formatted string\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' %kind)\n",
    "    \n",
    "    #Read in the labels\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8)) #Read the first 64 bits (8 bytes)\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8) # Construct array from Reading bytes one at a time\n",
    "        \n",
    "    #Read in the images\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784) #Read all the pixels into ONE large array, then reshape it into a 60,000 x 784 matrix\n",
    "        images = ((images / 255.) - 0.5) * 2 #Normalize pixels between -1 to 1\n",
    "        \n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f93843",
   "metadata": {},
   "source": [
    "It is common practice to have pixels fall in the $[-1, 1]$ range and centered at 0. It usually works well in practice, Sebastian Raschka says in his Python Machine Learning book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca706e",
   "metadata": {},
   "source": [
    "With the `load_mnist` helper written, let's load the MNNIST dataset. In my case, the MNIST dataset is within the `dataset` folder which I added to my `.gitignore` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f7edf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\billy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/billy/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#X_train = the training set, \n",
    "#y_train = label for each training example in X_train\n",
    "X_train, y_train = load_mnist('./dataset', kind='train')\n",
    "\n",
    "#X_train dimensions\n",
    "print('Rows: %d, Columns: %d' %(X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea1d86",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\billy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/billy/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#X_test = the testing set (part of the dataset set aside to test the NN)\n",
    "#y_test = label for each training example in X_test\n",
    "X_test, y_test = load_mnist('./dataset', kind='t10k')\n",
    "\n",
    "#X_test dimensions\n",
    "print('Rows: %d, Columns: %d' %(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0979a092",
   "metadata": {},
   "source": [
    "As we can see, the training set and the testing set contain 60,000 and 10,000 examples respectively. And each example has 784 features, where each feature is a pixel :). Since we are dealing with $28 \\times 28$ images, we have $784$ pixels in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108b74a",
   "metadata": {},
   "source": [
    "Let's plot some of the images using the Matplotlib's `imgshow` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02bb6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\billy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/billy/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5,\n",
    "                     sharex=True, sharey=True)\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2615fd1",
   "metadata": {},
   "source": [
    "Cool, right !? Let's move on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b1bf9",
   "metadata": {},
   "source": [
    "Those NumPy arrays are big. At least, from the eyes of a begineer like me. Surely, there should be a way to compress and store those arrays so we avoid the overhead of reading and processing the data over and over.\n",
    "\n",
    "To achieve that, NumPy provides a function called `savez_compressed`. It saves several arrays into a single in compressed `.npz` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52047d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\billy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/billy/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "np.savez_compressed('mnist_scaled.npz',\n",
    "                   X_train=X_train,\n",
    "                   y_train=y_train,\n",
    "                   X_test=X_test,\n",
    "                   y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37483283",
   "metadata": {},
   "source": [
    "After we create the `.npz` file, we can read it and load the preprocessed MNIST images using NumPy's `load` function. It goes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65957f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\billy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/billy/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "mnist = np.load('mnist_scaled.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02a632",
   "metadata": {},
   "source": [
    "The `load` function returns an object with some attributes, \"**files**\" is one of them. The \"files\" attribute returns a list of the objects inside. It goes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad4e7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\billy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/billy/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(mnist.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8449fa4",
   "metadata": {},
   "source": [
    "Let's do a recap of what happened so far because what will follow is new and important for me:\n",
    "\n",
    "1. We downloaded the MNIST dataset. It's a 4-part dataset\n",
    "2. We unzipped the archives\n",
    "3. Read the dataset into Numpy arrays using the `load_mnist` helper we wrote\n",
    "4. We compressed the NumPy arrays using the `savez_compressed` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc07949",
   "metadata": {},
   "source": [
    "Now, let's implement our MLP to classify those handwritten digits. This is what we are here for, right? So, buckle up 😊🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7319d8",
   "metadata": {},
   "source": [
    "### Implementing a multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a8be6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\billy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/billy/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    \"\"\"Feedforward neural network / Multi-layer perceptron classifier\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units\n",
    "    \n",
    "    l2 : float (default: 0.)\n",
    "        Lambda value for L2-regularization\n",
    "        No regularization if l2=0. (default)\n",
    "    \n",
    "    epochs : int (default: 100)\n",
    "        Number of passes over the training set\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate\n",
    "        \n",
    "    shuffle: bool (default: True)\n",
    "        Shuffles training data every epoch if \"True\" to prevent cycles\n",
    "        \n",
    "    minibatch_size : int (default: 1)\n",
    "        Number of training examples per batch. The number of training\n",
    "        examples to process before updating the weight. Remember the\n",
    "        definiton of \"batch\"\n",
    "        \n",
    "    seed : int (default: None)\n",
    "        Random seed for initializing weights and shuffling\n",
    "        \n",
    "    \n",
    "    Attributes\n",
    "    ------------\n",
    "    eval_ : dict\n",
    "        Dictionary of three arrays collecting the cost, training accuracy,\n",
    "        and validation accuracy (respectively) for each epoch during training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_hidden=30, l2=0.,\n",
    "                 epochs=100, eta=0.001, shuffle=True, minibatch_size=1, seed=None):\n",
    "        \n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "    def _oneshot(self, y, n_labels):\n",
    "        \"\"\"Encode labels into one-hot representation\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        y: array, shape = [n_examples]\n",
    "            Target values.\n",
    "        \n",
    "        n_labels: int\n",
    "            Number of labels. In our case, 10.\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        oneshot: array, shape = (n_examples, n_labels)\n",
    "        \"\"\"\n",
    "        \n",
    "        onehot = np.zeros((n_labels, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1\n",
    "        \n",
    "        return onehot.T\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\"\"\"\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"Compute forward propagation from input to output layer\"\"\"\n",
    "        \n",
    "        #1- Forward propagate from INPUT to HIDDEN layer\n",
    "        # \"Training matrix\" times \"weight matrix\"\n",
    "        # [n_examples, n_features] dot [n_features, n_hidden] -> [n_examples, n_hidden]\n",
    "        z_h = np.dot(X, self.w_h) + self.b_h\n",
    "        \n",
    "        #2- Apply activation function to hidden layer\n",
    "        a_h = self._sigmoid(z_h)\n",
    "        \n",
    "        #3- Forward propagate from HIDDEN to OUTPUT layer\n",
    "        #[n_examples, n_hidden] dot [n_hidden, n_classlabels] -> [n_examples, n_classlabels]\n",
    "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
    "        \n",
    "        #4- Apply activation function to output layer\n",
    "        a_out = self._sigmoid(z_out)\n",
    "        \n",
    "        return z_h, a_h, z_out, a_out\n",
    "    \n",
    "    def _compute_cost(self, y_enc, output):\n",
    "        \"\"\"Compute cost function\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        y_enc: array, shape = (n_examples, n_labels)\n",
    "            one-hot encoded class labels\n",
    "            \n",
    "        output: array, shape = [n_examples, n_output_units]\n",
    "            Activation of the output layer. Results from forward propagating from hidden to output layer\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        cost : float\n",
    "            Regularized cost\n",
    "        \"\"\"\n",
    "        L2_term = (self.l2 * \n",
    "                   (np.sum(self.w_h ** 2.0) +\n",
    "                    np.sum(self.w_out ** 2.0)))\n",
    "        \n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2) + L2_term\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X : array, shape = [n_examples, n_features]\n",
    "            Training set\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        y_pred : array, shape = [n_examples]\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        \n",
    "        z_h, a_h, z_out, a_out = self._forward(X)\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\" Learn weights from training data\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X_train : array, shape = [n_examples, n_features]\n",
    "            Input layer with original features. It's our training set :)\n",
    "        y_train : array, shape = [n_examples]\n",
    "            Target class labels for each training example\n",
    "        X_valid : array, shape = [n_examples, n_features]\n",
    "            Part of the training set used for validation during training\n",
    "        y_valid : array, shape = [n_examples]\n",
    "            Target class labels for each training examples used for validation during training\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        self\n",
    "        \n",
    "        \"\"\"\n",
    "        n_output = np.unique(y_train).shape[0] # no. of class labels. 10 in our case\n",
    "        n_features = X_train.shape[1] #784 features for each training example\n",
    "        \n",
    "        ########################\n",
    "        # Weight initialization\n",
    "        ########################\n",
    "        \n",
    "        #1- Weight matrix from input -> hidden layer\n",
    "        self.b_h = np.zeros(self.n_hidden)\n",
    "        self.w_h = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden))\n",
    "        \n",
    "        #2- Weight matrix from hidden -> output layer\n",
    "        self.b_out = np.zeros(self.n_output)\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output))\n",
    "        \n",
    "        ########################\n",
    "        # Some setup before training\n",
    "        ########################\n",
    "        \n",
    "        #1- for progr. format.\n",
    "        epoch_strlen = len(str(self.epochs))\n",
    "        \n",
    "        #2- collects the \"cost\", \"training\", and \"validation\" accuracy for each epoch\n",
    "        self.eval_ = {\n",
    "            'cost': [],\n",
    "            'train_acc': [],\n",
    "            'valid_acc': []\n",
    "        }\n",
    "        \n",
    "        #3- Hot encode training labels. Returns a \"n\" x 10 matrix\n",
    "        y_train_enc = self._oneshot(y_train, n_output)\n",
    "        \n",
    "        ########################\n",
    "        # Now training\n",
    "        ########################\n",
    "        for i in range(self.epochs):\n",
    "            #1- Generate an array of indices from 0 to X_train.shape[0] - 1 (n examples)\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            \n",
    "            #2- Shuffle the array\n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "                \n",
    "            #3- For each \"portion\" of the training set\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                \n",
    "                #4- Select a range of indices from the \"indices\" array\n",
    "                batch_idx = indices[start_idx: start_idx + self.minibatch_size]\n",
    "                \n",
    "                #5- Forward propagate the training examples whose indices are in \"batch_idx\" through the NN\n",
    "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
    "                \n",
    "                ########################\n",
    "                # Backpropagation\n",
    "                ########################\n",
    "                \n",
    "                #6- The Delta out expression appears when taking\n",
    "                # the partial derivative of the loss function with respect to EACH \n",
    "                # weights in w_out and w_h. \n",
    "                # See my hand written work in \"etc\" folder.\n",
    "\n",
    "                #delta_out is a matrix [n_examples, n_classlabels]\n",
    "                delta_out = a_out - y_train_enc[batch_idx]\n",
    "\n",
    "                #7- The derivative of the activation with respect to z_h appears \n",
    "                # when taking partial derivatives of the loss function \n",
    "                # See my hand written work in \"etc\" folder.\n",
    "                #[n_examples, n_classlabels]\n",
    "                sigmoid_derivative_h = a_h * (1. - a_h)\n",
    "\n",
    "                #8- The delta_h exp appears when taking the derivative of the loss\n",
    "                # function with respect to weights in W_h\n",
    "                # [n_examples, n_classlabels] dot [n_classlabels, n_hidden] --> [n_examples, n_hidden]\n",
    "                delta_h = (np.dot(delta_out, self.w_out.T) * sigmoid_derivative_h)\n",
    "\n",
    "                #9- Compute the gradient of w_h\n",
    "                grad_w_h = np.dot(X_train[batch_idx].T, delta_h)\n",
    "                grad_b_h = np.sum(delta_h, axis=0)\n",
    "\n",
    "                #10- Compute the gradient of w_out\n",
    "                grad_w_out = np.dot(a_h.T, delta_out)\n",
    "                grad_b_out = np.sum(delta_out, axis=0)\n",
    "\n",
    "                #11- Regularization and weight updates\n",
    "                delta_w_h = (grad_w_h + self.l2*self.w_h)\n",
    "                delta_b_h = grad_b_h # bias is not regularized\n",
    "                self.w_h -= self.eta * delta_w_h\n",
    "                self.b_h -= self.eta * delta_b_h\n",
    "\n",
    "                delta_w_out = (grad_w_out + self.l2*self.w_out)\n",
    "                delta_b_out = grad_b_out # bias is not regularized\n",
    "                self.w_out -= self.eta * delta_w_out\n",
    "                self.b_out -= self.eta * delta_b_out\n",
    "\n",
    "            #############\n",
    "            # Evaluation\n",
    "            #############\n",
    "\n",
    "            #12- Forward propagate X_train through the network\n",
    "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
    "\n",
    "            #13- Compute the cost for this epoch\n",
    "            cost = self._compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "\n",
    "            #14- \n",
    "            y_train_pred = self.predict(X_train)\n",
    "            y_valid_pred = self.predict(X_valid)\n",
    "\n",
    "            #15- Compute model accuracy on training and validation sets for this epoch\n",
    "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) / X_train.shape[0])\n",
    "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) / X_valid.shape[0])\n",
    "\n",
    "            #16-\n",
    "            sys.stderr.write('\\r%0*d/%d | Cost: %.2f ' '| Train/Valid Acc.: %.2f%%/%.2f%% ' %(epoch_strlen, i+1, self.epochs, cost, train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "\n",
    "            #17- \n",
    "            self.eval_['cost'].append(cost)\n",
    "            self.eval_['train_acc'].append(train_acc)\n",
    "            self.eval_['valid_acc'].append(valid_acc)\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d518651b9284ef9ca771e20a1baeb96db7651c9e6d8999808eec147539789db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
