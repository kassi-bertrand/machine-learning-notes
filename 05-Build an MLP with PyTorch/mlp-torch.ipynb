{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e97c768",
   "metadata": {},
   "source": [
    "# MLP with PyTorch\n",
    "\n",
    "At this point, I know what MLPs are. We built one from scratch one of the previous folder. In this notebook, I want to implement an MLP again, but using PyTorch. This MLP will trained and evaluated on the MNIST dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95314f9",
   "metadata": {},
   "source": [
    "# The MNIST dataset\n",
    "\n",
    "Conveniently, the MNIST dataset is provided in PyTorch through the `torchvision` module, specifically through the `torchvision.dataset` module.\n",
    "\n",
    "In the following cell, I import the `torchvision` and `transforms` modules. The second module, as the name suggests, let us perform **common transformations on image data**. According to the [documentation](https://pytroch.org/vision/stable/transforms.html), Transforms are common image transformations available in the `torchvision.transforms` module.\n",
    "\n",
    "Another interesting feature is that transform operations can be **chained** together using `Compose`. We will use it in a couple of cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c73c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4253a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484c6d5",
   "metadata": {},
   "source": [
    "With the modules loaded, I want to load the dataset itself, and specify hyperparameters such as the size of the training and testing sets, and size of the mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99200ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7b1967c250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = './'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mnist_train_dataset = torchvision.datasets.MNIST(\n",
    "    root=image_path, train=True,\n",
    "    transform=transform, download=True \n",
    ")\n",
    "\n",
    "mnist_test_dataset = torchvision.datasets.MNIST(\n",
    "    root=image_path, train=False,\n",
    "    transform=transform, download=True  \n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c7899",
   "metadata": {},
   "source": [
    "Okay, what just happened? Since I want to download a dataset I created a `image_path` variable to store the path where I would like images to be stored, should they be downloaded or read from the filesystem, if I do not want the dataset to be downloaded.\n",
    "\n",
    "Then I move on create a `transform` pipeline. Ours only has one operation: `transform.ToTensor()`. The `ToTensor()` method (1)converts the pixel features into a floating type tensor and (2) normalizes the pixel from range [0, 255] to range [0, 1].\n",
    "\n",
    "After that is where I effectively create the training and testing dataset using the MNIST dataset. Since, I do not have it on my machine, I asked PyTorch to download it for me using the `download` paramater. I also want PyTorch to perform the `transform` we created earlier on the images being downloaded. I specify which operation to perform using the `transform` paramater.\n",
    "\n",
    "I finish with specifying the batch size, and manually setting the seed number of random number generation.\n",
    "\n",
    "With that being done, we cannot use the dataset just yet. We must pass the `Dataset` objects (`mnist_train_dataset` and `mnist_test_dataset`) into a dataset a `DataLoader` object. Remember through a `DataLoader`, we can properly iterate over a given dataset. Okay, let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf5fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(mnist_train_dataset,\n",
    "                      batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b962fe",
   "metadata": {},
   "source": [
    "We successfully created the data loader, with batches of 64 samples. Let's move on :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b769876",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "This section of the notebook is concerned with building the MLP to classify digits from the dataset we downloaded earlier.\n",
    "\n",
    "Our MLP will have:\n",
    "\n",
    "- an input layer\n",
    "- a hidden layer (32 activation units)\n",
    "- a hidden layer (16 activation units)\n",
    "- an output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2ac1a",
   "metadata": {},
   "source": [
    "Let's define the above layers in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b11fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [32, 16] #number of activation units in EACH hidden layer\n",
    "image_size = mnist_train_dataset[0][0].shape #mnist_train_dataset[0] is a tuple (image[tensor], label)\n",
    "input_size = image_size[0] * image_size[1] * image_size[2] #number of channels * image height * image width\n",
    "\n",
    "# all the layers in the network \n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit  in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10)) #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd48ea",
   "metadata": {},
   "source": [
    "We successfully created all the layers in the network, and we stored them all into the an array called: `all_layers`.\n",
    "\n",
    "Let's now create a model containing the layers we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5cb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*all_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3e7a4",
   "metadata": {},
   "source": [
    "That's it. Done! Since each layer comes one after the other in an MLP, we use the `torch.nn.Sequential` module to place those layers *sequentially*.\n",
    "\n",
    "Let's print the model and see all the layers we inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44aa88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004cc03",
   "metadata": {},
   "source": [
    "When an image, represented by a $(1 \\times 28 \\times 28)$ vector, gets fed into the network  the `Flatten` layer flattens it to a $(1 \\times 784)$ vector.\n",
    "\n",
    "This flattened vector goes through the first `Linear` layer. This is the **input** layer. It turns the $(1 \\times 784)$ to a $(1 \\times 32)$ vector. `(1)`\n",
    "\n",
    "Right after that, the vector goes through a `RELU` activation function. The size of the vector remains the same: $(1 \\times 32)$\n",
    "\n",
    "The $(1 \\times 32)$ vector goes through a second `Linear` layer, which downsizes it to a $(1 \\times 16)$ vector. This vector also goes through, a `RELU` activation.\n",
    "\n",
    "Lastly, the $(1 \\times 16)$ vector goes through the last `Linear` layer. The **output** layer. Which turns the $(1 \\times 16)$ vector into a $(1 \\times 10)$ vector. Which is our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4ff74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#moves model to the gpu\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f6025",
   "metadata": {},
   "source": [
    "Let's now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91cb42bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Accuracy: 0.8514\n",
      "Epoch 1 Accuracy: 0.9287\n",
      "Epoch 2 Accuracy: 0.9422\n",
      "Epoch 3 Accuracy: 0.9492\n",
      "Epoch 4 Accuracy: 0.9538\n",
      "Epoch 5 Accuracy: 0.9584\n",
      "Epoch 6 Accuracy: 0.9622\n",
      "Epoch 7 Accuracy: 0.9647\n",
      "Epoch 8 Accuracy: 0.9673\n",
      "Epoch 9 Accuracy: 0.9688\n",
      "Epoch 10 Accuracy: 0.9710\n",
      "Epoch 11 Accuracy: 0.9720\n",
      "Epoch 12 Accuracy: 0.9737\n",
      "Epoch 13 Accuracy: 0.9750\n",
      "Epoch 14 Accuracy: 0.9769\n",
      "Epoch 15 Accuracy: 0.9780\n",
      "Epoch 16 Accuracy: 0.9788\n",
      "Epoch 17 Accuracy: 0.9799\n",
      "Epoch 18 Accuracy: 0.9814\n",
      "Epoch 19 Accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    accuracy_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        #moves the batch to the gpu so it can be there with the model\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        #compute forward pass and loss value\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        \n",
    "        #compute gradients through backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        #update weights based on gradients, then reset the gradients to zero \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #how correct was our model on this batch?\n",
    "        is_correct = (\n",
    "            torch.argmax(pred, dim=1) == y_batch\n",
    "        ).float()\n",
    "\n",
    "        #Number of times model was correct on this batch\n",
    "        accuracy_hist_train += is_correct.sum()\n",
    "    \n",
    "    accuracy_hist_train /= len(train_dl.dataset)\n",
    "    print(f'Epoch {epoch} Accuracy: '\n",
    "          f'{accuracy_hist_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a459235",
   "metadata": {},
   "source": [
    "Cool, our model finished training. The model acheived a 98% accuracy on training set, but let's see how well it performs on the testing set which is data it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80518c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test set: 0.9610\n"
     ]
    }
   ],
   "source": [
    "pred = model(mnist_test_dataset.data.float().to(device))\n",
    "\n",
    "is_correct = (\n",
    "    torch.argmax(pred, dim=1) == mnist_test_dataset.targets.to(device)\n",
    ").float()\n",
    "\n",
    "print(f'Accuracy on Test set: {is_correct.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d98605",
   "metadata": {},
   "source": [
    "That's it!\n",
    "\n",
    "We successfully trained and evaluated our model on the MNIST dataset. The performance on the training set is 2% higher, than the performance on the testing set. I think the model generalizes fairly well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "151ee98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4759e+01, -1.7047e+01, -2.0928e-01, -6.8313e+00, -1.2147e+01,\n",
       "         -8.0281e+00,  6.3435e-01, -5.0821e+00, -4.6575e+00, -5.3072e+00],\n",
       "        [-7.0193e+00, -1.5395e+01, -7.0325e+00, -3.8280e+00,  1.4617e-01,\n",
       "         -2.1121e+00, -1.3263e+01, -2.7679e+00, -1.2906e+00,  3.2976e+00],\n",
       "        [-1.1946e+01, -5.2218e+00,  1.0317e+00,  1.3162e+00, -1.9276e+01,\n",
       "         -1.7503e+01, -3.4389e+01,  1.2010e+01, -7.3719e+00, -5.3010e+00],\n",
       "        [-9.4134e+00, -7.6419e+00, -7.7214e+00, -3.6080e+00,  3.5553e+00,\n",
       "         -3.3164e+00, -5.1598e+00, -1.0543e+01,  8.3371e+00, -7.1106e+00],\n",
       "        [-1.5993e+01, -7.3527e+00, -1.0119e+01, -1.1165e+01, -2.4158e+00,\n",
       "          4.4177e+00, -3.0337e+00, -1.5408e+01,  1.4155e+01, -7.6252e+00],\n",
       "        [-7.2085e+00, -6.5058e+00, -6.3147e+00, -1.0603e+01,  1.7418e+01,\n",
       "         -6.7194e+00, -8.4649e+00, -6.6750e-01, -5.0204e+00, -1.3512e+00],\n",
       "        [-9.4327e+00,  8.8173e+00,  6.9889e-01, -2.5305e+00, -7.7006e+00,\n",
       "         -1.1856e+01, -9.7926e+00,  4.9008e+00, -2.0255e+00, -1.4932e+01],\n",
       "        [-9.1371e+00, -5.9422e+00, -8.2172e+00,  3.8639e+00, -1.9121e+01,\n",
       "          9.0776e+00, -1.5927e+01, -1.6629e+01,  1.4501e-01,  5.0870e+00],\n",
       "        [-1.1104e+01,  6.6206e+00, -1.5232e+00, -3.1047e+00, -8.4015e+00,\n",
       "         -3.6994e+00, -3.6272e+00, -2.9288e+00,  1.0584e-01, -8.2532e+00],\n",
       "        [-9.0759e+00, -3.4267e+00, -7.9227e+00, -8.0271e+00, -1.0218e+01,\n",
       "          1.0768e+00, -7.2503e+00, -1.1531e+01,  9.6364e+00, -4.5143e+00],\n",
       "        [-9.2235e+00, -6.5337e+00, -4.1159e+00, -2.4302e-01, -1.7221e+01,\n",
       "         -1.0072e+01, -2.5978e+01,  4.8264e+00, -1.2887e+00, -9.6804e+00],\n",
       "        [-7.5249e+00, -3.4526e+00, -2.8742e+00, -5.6782e+00, -5.3281e+00,\n",
       "          8.9501e-01,  1.1460e+01, -1.7678e+01, -1.0031e+00, -1.4841e+01],\n",
       "        [-5.8875e+00, -1.1495e+01, -2.8286e-01,  2.3893e+00, -1.1645e+01,\n",
       "         -6.1690e+00, -1.3567e+01, -1.6239e+01,  1.9753e+00, -6.5897e+00],\n",
       "        [-1.0617e+01, -1.2179e+01, -1.1242e+01,  3.2996e+00, -4.4594e+00,\n",
       "         -1.5147e+00, -2.8600e+01, -3.4867e+00, -2.8927e+00,  1.2899e+01],\n",
       "        [-2.1503e+01,  4.6852e+00, -7.1072e+00, -2.3356e+00, -5.1239e-01,\n",
       "         -7.8810e+00, -1.1580e+01, -1.0661e+01, -4.4504e-01, -4.1532e+00],\n",
       "        [-1.5922e+01, -1.1228e+01, -2.6271e-01, -1.3879e+00, -1.9497e+01,\n",
       "         -2.0026e+01, -4.2024e+01,  1.4993e+01, -6.8016e+00, -2.3526e+00],\n",
       "        [-2.5153e+01,  5.5075e+00, -6.1177e+00,  8.9198e-01, -1.1352e+01,\n",
       "         -1.5427e+00, -1.0056e+01, -1.6119e+01,  6.6896e-01, -9.8705e+00],\n",
       "        [-1.5387e+01, -1.2761e+01, -1.2568e+00, -6.1715e+00, -1.7546e+01,\n",
       "          3.9641e+00, -1.3569e+00, -1.8760e+01,  6.3370e+00, -1.0229e+01],\n",
       "        [-6.6820e+00, -9.8432e+00, -6.9823e+00, -4.2343e+00, -2.4431e+00,\n",
       "         -1.8705e+01, -2.3897e+01, -2.1728e+00, -4.7627e+00,  9.0610e+00],\n",
       "        [-1.1568e+01, -8.9303e+00, -1.1195e+01, -7.7836e+00,  1.5473e+01,\n",
       "         -1.2441e+01, -1.8148e+01, -2.3701e-01, -4.1459e+00,  6.2151e+00],\n",
       "        [-9.7036e+00, -6.1234e+00, -6.2871e+00,  1.1626e+01, -1.1411e+01,\n",
       "         -3.7738e+00, -1.2585e+01, -1.2163e+01, -6.9700e+00, -5.9421e+00],\n",
       "        [-2.4099e+01, -1.6573e+01, -5.7156e+00,  1.0741e+01, -3.5446e+01,\n",
       "          2.2843e+00, -3.2197e+01, -2.8835e+01,  5.0400e+00, -6.3916e+00],\n",
       "        [-9.0527e+00, -7.7422e+00, -7.2350e+00,  7.0941e+00, -2.2247e+01,\n",
       "          4.8316e+00, -1.9447e+01, -1.0134e+01, -4.0939e+00,  2.0795e+00],\n",
       "        [-1.6850e+01, -1.0645e+01, -1.4884e+00,  9.6321e+00, -1.5845e+01,\n",
       "         -7.0336e+00, -2.7718e+01, -8.3499e+00, -4.1102e+00, -7.6149e+00],\n",
       "        [-8.4334e+00, -9.2098e+00, -1.1998e+01,  4.3348e-01, -4.9151e+00,\n",
       "         -2.3216e+00, -1.2371e+01, -1.6028e+01,  5.9598e+00,  1.7027e+00],\n",
       "        [-5.5648e+00, -1.4029e+01,  6.6184e+00, -5.0401e-01, -2.1838e+01,\n",
       "         -9.4673e+00, -1.3042e+01, -4.7113e+00, -2.0482e+00, -1.0161e+01],\n",
       "        [-1.3605e+01,  7.4618e+00, -9.4638e-01, -4.1596e+00, -1.1280e+01,\n",
       "         -1.0297e+01, -9.5895e+00,  9.6803e-01,  1.4023e-01, -1.2208e+01],\n",
       "        [-2.0798e+01, -1.4239e+01, -1.1662e+01,  1.4490e+01, -3.0411e+01,\n",
       "         -1.5224e+00, -2.4469e+01, -2.8141e+01, -2.1174e+00, -1.5636e+01],\n",
       "        [-1.3625e+01, -1.6768e+01, -1.3874e+01,  1.0521e+01, -1.9763e+01,\n",
       "         -1.1801e+00, -2.7184e+01, -2.8698e+01,  1.5947e+00, -2.0354e-01],\n",
       "        [-6.3657e+00, -2.1456e+00,  3.5075e+00, -5.4925e+00, -1.5474e+01,\n",
       "         -9.9962e+00, -1.0128e+01,  5.6577e+00, -8.4269e-01, -8.0525e+00],\n",
       "        [-2.1658e+01, -1.4619e+01, -7.8937e+00,  1.6038e+01, -3.1597e+01,\n",
       "          2.9093e+00, -3.1304e+01, -2.5690e+01, -2.8688e+00, -6.3430e+00],\n",
       "        [-1.1591e+01, -5.7469e+00, -4.8723e+00,  4.1002e+00, -3.2857e+01,\n",
       "          8.8929e-01, -2.2214e+01, -1.5419e+01, -1.0692e+00, -1.7109e+00],\n",
       "        [-1.2244e+01, -1.5902e+01, -7.4145e+00, -5.1752e+00,  8.3529e+00,\n",
       "         -1.0221e+01, -1.6549e+01, -1.8242e+00, -1.2789e+00, -7.8040e-01],\n",
       "        [-7.1628e+00, -2.6546e+00, -1.1101e+01, -1.9497e+00,  5.7371e+00,\n",
       "         -8.4597e+00, -1.5181e+01, -4.6459e+00, -9.2906e-01,  8.9998e+00],\n",
       "        [-1.0324e+01, -5.2546e+00, -2.2583e+00,  6.4730e-01, -1.2756e+01,\n",
       "         -1.2706e+01, -2.8028e+01,  9.5938e+00, -6.9962e+00,  2.6473e-02],\n",
       "        [-1.0479e+01, -1.8885e+01, -1.2852e+01,  3.6301e+00, -2.4156e+01,\n",
       "          1.6112e+01, -7.0657e+00, -3.7400e+01, -1.0307e+00, -4.6854e+00],\n",
       "        [-1.5535e+01, -5.6578e-01,  7.7971e+00, -3.0773e+00, -2.7177e+01,\n",
       "         -1.0110e+01, -1.2692e+01, -1.2508e+01,  9.2954e-01, -1.6655e+01],\n",
       "        [-8.6658e+00, -3.1470e+00,  4.7953e+00, -1.7309e+00, -3.5460e+00,\n",
       "         -9.9191e+00, -8.9041e+00,  5.6483e-01, -1.0935e+00, -1.1637e+01],\n",
       "        [-7.1989e+00, -9.4473e+00, -8.1629e+00, -3.5788e+00, -1.8297e+00,\n",
       "         -6.1683e+00, -1.6718e+01, -2.0488e+00, -2.3433e+00,  6.1973e+00],\n",
       "        [-9.0016e+00, -4.5783e+00,  8.5293e-01, -1.7937e+00, -1.6040e+01,\n",
       "         -1.3291e+01, -1.8559e+01,  5.9765e+00, -4.4375e+00, -7.6600e+00],\n",
       "        [-8.4524e+00, -1.5019e+01,  1.8324e+01, -1.1710e+00, -1.7949e+01,\n",
       "         -1.0063e+01, -9.2153e+00, -6.6139e+00, -7.0682e+00, -2.3299e+01],\n",
       "        [-1.3380e+01,  1.0159e+01, -7.4320e-01, -3.5033e+00, -1.1140e+01,\n",
       "         -1.0781e+01, -6.8624e+00, -2.0925e+00,  2.8532e-01, -8.8935e+00],\n",
       "        [-1.5830e+01,  8.6683e+00, -2.7891e+00, -1.4236e+00, -7.7452e+00,\n",
       "         -8.8274e+00, -7.0848e+00, -4.8029e+00, -1.3333e+00, -6.8976e+00],\n",
       "        [-6.2441e+00, -7.1404e+00, -3.7569e+00, -7.6474e-01, -1.8362e+01,\n",
       "          9.3258e+00, -1.5807e+01,  2.4620e+00, -7.5567e-01, -5.3220e+00],\n",
       "        [-4.5114e+00, -4.6520e+00, -3.1556e+00, -1.9287e+00, -7.7091e+00,\n",
       "         -7.6162e+00, -2.3476e+01,  1.2082e+01, -6.8419e+00,  5.5925e+00],\n",
       "        [-4.4851e+00, -1.1540e+01, -2.1260e+00,  7.6416e-02, -1.7475e+01,\n",
       "         -8.7981e+00, -2.3454e+01,  7.0190e+00, -5.3409e+00,  2.1166e+00],\n",
       "        [-1.6352e+01, -1.4950e+01, -4.7516e+00,  1.2020e+01, -1.7458e+01,\n",
       "         -5.5466e+00, -2.9219e+01, -1.4295e+01, -2.8270e+00, -4.9189e+00],\n",
       "        [-1.8491e+01, -6.8817e+00,  1.4388e+01,  2.0661e+00, -3.2582e+01,\n",
       "         -8.0667e+00, -1.2574e+01, -4.1997e+00, -3.5565e+00, -3.1821e+01],\n",
       "        [-1.4445e+01, -1.8662e+01, -6.4904e+00, -2.3241e+00, -2.1928e+01,\n",
       "         -1.0457e+01, -3.1775e+01, -8.0373e-01,  2.9216e+00, -4.5160e+00],\n",
       "        [-1.0468e+01,  7.0216e+00,  1.6598e+00, -4.8603e+00, -1.3216e+01,\n",
       "         -2.6983e+00, -3.4495e+00, -1.8707e+00,  8.2795e-01, -1.3733e+01],\n",
       "        [-1.5883e+01,  7.2772e+00, -4.4217e+00, -6.5868e-01, -8.8349e+00,\n",
       "         -6.6924e+00, -8.1253e+00, -6.9311e+00, -1.3769e+00, -6.7217e+00],\n",
       "        [-1.3048e+01, -1.0895e+01, -7.3166e+00,  2.3177e+00, -1.7028e+01,\n",
       "          9.7219e+00, -1.2467e+01, -1.8824e+01,  1.7701e-01, -5.3657e-01],\n",
       "        [-5.8809e+00, -9.1654e+00, -5.2295e-01,  2.8175e+00, -1.0054e+01,\n",
       "         -8.7375e+00, -2.5027e+01,  7.9987e+00, -6.1124e+00,  1.1254e+00],\n",
       "        [-1.0533e+01, -1.0536e+01, -7.4872e+00,  1.2876e+01, -2.6370e+01,\n",
       "          1.4023e+00, -1.8566e+01, -2.1879e+01, -2.5111e+00, -6.6521e+00],\n",
       "        [-1.7456e+01, -9.5131e-01,  1.5665e+01,  3.2299e+00, -2.1102e+01,\n",
       "         -9.7636e+00, -4.3928e+00, -1.4281e+01, -4.6485e+00, -3.1117e+01],\n",
       "        [-5.4592e+00, -5.4156e+00, -1.1155e+01, -7.2283e+00,  4.6985e+00,\n",
       "         -7.7508e+00, -1.2122e+01, -6.0741e+00, -2.5931e+00,  1.0700e+01],\n",
       "        [-1.1321e+01, -1.1371e+01,  1.5766e+01,  2.7645e+00, -1.6118e+01,\n",
       "         -8.9441e+00, -1.1565e+01, -7.8517e+00, -7.0290e+00, -2.2680e+01],\n",
       "        [-8.2378e+00, -1.0627e+01,  3.4562e+00,  8.1589e-01, -2.3931e+01,\n",
       "         -1.5192e+01, -3.4825e+01,  1.2630e+01, -8.1661e+00, -2.3384e+00],\n",
       "        [-7.3007e+00, -1.3194e+00,  6.3634e+00, -5.7658e-01,  8.2291e-01,\n",
       "         -9.8563e+00, -7.7864e+00,  6.3497e-01, -5.0808e+00, -6.0675e+00],\n",
       "        [-1.1110e+01, -1.6835e+01, -1.5402e+01,  5.5823e+00, -2.3167e+01,\n",
       "          1.7248e+01, -1.1744e+01, -3.1780e+01,  4.1704e+00, -3.3771e+00],\n",
       "        [-1.0378e+01,  8.3533e+00, -1.3625e+00, -5.0635e+00, -8.2476e+00,\n",
       "         -6.4128e+00, -3.4493e+00, -2.3579e+00,  4.3324e-01, -1.2223e+01],\n",
       "        [-7.0381e+00,  1.3917e+00,  1.4405e+00, -3.3501e+00,  7.5056e-01,\n",
       "         -9.5379e+00, -8.8142e+00,  2.0107e+00, -3.1133e+00, -5.1967e+00],\n",
       "        [-1.4742e+01, -1.0379e+01, -9.4299e+00,  1.2133e+01, -1.3373e+01,\n",
       "          7.7826e-01, -1.8870e+01, -1.6772e+01, -4.0275e+00, -4.5246e+00],\n",
       "        [ 5.2374e+00, -1.2628e+01, -3.9600e+00, -1.0591e+01, -9.1556e+00,\n",
       "         -7.4713e+00, -4.1243e+00, -5.5664e-01, -6.1775e+00, -8.7162e-01]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dl = DataLoader(mnist_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "x, y = next(iter(test_dl))\n",
    "x, y = x.to(device), y.to(device)\n",
    "pred = model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a7e43",
   "metadata": {},
   "source": [
    "#  Potential Experiments\n",
    "\n",
    "Here are few ideas to further the exploration:\n",
    "\n",
    "1. Feed foward only ONE training example through the network. What is the shape of the output vector? Can you also print its values?\n",
    "\n",
    "2. We used `RELU` as our activation function. Modify the model so it uses sigmoid instead. Does it affect the model performance on the testing set?\n",
    "\n",
    "3. Using the model we built, can you reset it and graph the weights as the model learns. See how they change over time? Might be a cool visual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
