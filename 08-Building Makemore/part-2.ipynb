{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is the second in the Makemore series. In part 1 we implemented Makemore using a bigram model. We looked at ONE previous character and produced a probability distribution of what character is likely to come next. We did that using two different approaches and reached the same results. First, we used counts and normalized them into probabilities. Second, we used a simple neural network (i.e. linear layer) to produce those same probabilities.\n",
    "\n",
    "The limit of our implementation in part 1 is that it only looks at ONE previous character; and because the bigram model took only ONE character as context, the prediction of the bigram were **not** good. If we were to use consider MORE than one character when predicting the next one, the approach we used in part-1 (storing everything in a tensor) becomes unsustainable because _the number of combinations we will have to store in a tensor grows exponentially with the amount characters we take as context_. For instance, if we were to take just three characters as context when making a prediction, we would have to store $27 \\times 27 \\times 27 = 19683$ possibilities in a tensor. That's way to many possibilities, the majority of these possibilities will have very few counts, Andrej said.\n",
    "\n",
    "That's why for this second part, we are moving away from the Bigram model. This time, we are implementing a Multi-Layer Percepton to predict the next character. The modeling approach we are going to adopt follows the paper: [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengio et al. 2003 (MLP language model) paper walkthrough\n",
    "\n",
    "This paper, Andrej said, is not the first to have proposed the use of MLPs to predict the next character/token in a sequence, but was very influencial and is very often cited. Since the paper is long ($19$ pages) Andrej decided to give us gist of it, but invited us to read the entire work. This paper is what we are going to implement in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-building our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # For making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary of characters and mapping from/to integers. It is important to recall that we are building a character-level language model, unlike the paper who is describing a word-level language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary of characters and mapping to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i + 1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to set up the dataset such that we can feed training examples to the neural network easily. We are going to refurbish code we wrote in the previous tutorial. To form the dataset in the first part, we added the first part of the bigram (the context) in a tensor, `xs`... and the second part (the correct prediction) in another tensor, `ys`. \n",
    "\n",
    "We are doing something very similar here, but this time Andrej is adding MORE than ONE character as context. `X` will store the input of the neural network and `Y` will store the correct labels. If, out of curiosity, you uncomment the commented lines, the code print all the training examples _per_ word in our dataset. For the output to be manageable, I suggest doing it for a couple words and not the entire dataset ðŸ˜‰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "  #print(w)\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    idx = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(idx)\n",
    "    #print(''.join(itos[i] for i in context), '--->', itos[idx])\n",
    "    context = context[1:] + [idx] # crop and append\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the dataset is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set `X` contains $32$ training examples of $3$ characters (i.e. context) each when **considering only $5$ words**. If we were to consider all of the words, our training set would contain $228146$ training examples, each with a context size of $3$. Also, printing the `dtype` indicates that tensors `X` and `Y` are storing `int64` values. It is because we are not storing the characters directly but rather the unique integers we assigned to each of them using `stoi` defined earlier in this notebook.\n",
    "\n",
    "Feel free to print out `X` or `Y` and see what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the embedding lookup table\n",
    "\n",
    "We have 27 possible characters and we are going to embed each of them in a lower dimensional space. In the paper, they have $17,000$ total words and embed them all into a 30 dimensional space, which Andrej said was small. Since we have just 27 characters like mentioned earlier, Andrej suggested we start with 2D embedding space. That means, each of the 27 letters will be associated with a 2D embedding vector. As a result, our embedding matrix will be of the shape $(27 \\times 2)$. In that sense, it is reasonable to look at the embedding matrix as a **lookup table**.\n",
    "\n",
    "_REMEMBER: Those embedding vectors are (initially) randomly generated._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4134, -0.5302],\n",
       "        [ 0.9838,  0.3194],\n",
       "        [ 1.2051,  1.5977],\n",
       "        [-1.1412, -0.0811],\n",
       "        [ 0.0588,  1.5821],\n",
       "        [ 1.2801, -0.1076],\n",
       "        [ 0.9547, -0.9674],\n",
       "        [ 0.5465,  0.2948],\n",
       "        [ 0.1205, -0.9544],\n",
       "        [ 0.6281,  0.2684],\n",
       "        [ 0.1358, -0.6574],\n",
       "        [-0.1792,  0.0842],\n",
       "        [-1.1114, -0.2912],\n",
       "        [ 1.3273,  1.0913],\n",
       "        [ 0.6448,  0.9663],\n",
       "        [ 0.4660, -0.6404],\n",
       "        [-2.0544, -0.7202],\n",
       "        [ 0.1043,  0.6912],\n",
       "        [ 0.5336,  0.0797],\n",
       "        [-0.9109,  0.9271],\n",
       "        [-1.1954,  2.4204],\n",
       "        [-1.2670,  1.8965],\n",
       "        [-1.1607, -0.7553],\n",
       "        [ 1.2723, -1.1243],\n",
       "        [ 1.5553, -0.1574],\n",
       "        [-0.4383,  1.9352],\n",
       "        [ 0.8560, -0.2054]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to index into the embedding matrix `C`. We can do it by using directly using the row index, or more interestingly, we can select specific rows of the matrix by multiplying it with a one-hot encoded vector. We illustrated this in \"NOTE 1\" section of the `part-1` notebook. For simplicity, Andrej decided to use numbers for indexing. With Pytorch, it is possible to do single-dimension indexing, meaning use a single number to select a row in a tensor or use a 1-D tensor to select multiple rows at once. Andrej showed us that is also possible to select rows in a tensor using a 2-D tensor.\n",
    "\n",
    "Using 2-D indexing, we can simultaneously retreive the embedding vector of all the integers in the training set `X` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we get a 3D matrix back? Remember each character is represented by a number in tensor `X`. For each number in `X` the corresponding embedding is returned. Since we have three numbers per row in `X`, the  is printed as a _series of $(3 \\times 2)$ matrices_. Makes sense? That's my way of looking at it.\n",
    "\n",
    "Andrej for instance said, for each of the elements in the $(32 \\times 3)$ matrix (i.e. the training set `X`), we retreived the corresponding the embedding. Makes sense?\n",
    "\n",
    "With the embedding of all the integers in `X` selected, Andrej stored them in the `embed` variable like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = C[X]\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the hidden layer + internals of torch. Tensor: storage, views\n",
    "\n",
    "In this section of the video, we implement the hidden layer of the of the neural network. Andrej decided to call the weights of this hidden layer `W1`, so we are going to do just like him.\n",
    "\n",
    "What should be the shape of `W1`? It depends on two things. (1) the number of neurons in the hidden layer. This number determines the number columns in `W1`. (2) the number of inputs per neurons. This number will determine the number of rows in `W1`.\n",
    "\n",
    "The number of inputs to the hidden layer $3 \\times 2$ which is **$6$**. Why? Because we are feeding three integers(i.e. characters) at a time and each of these integers are associated with a 2-D embedding vector. And the number of neurons in the hidden layer is a hyperparameter, meaning it is something the designer of the network has to choose. Andrej decided to go with $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100) #biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would like to do something like this:\n",
    "\n",
    "```py\n",
    "embed @ W1 + b\n",
    "```\n",
    "\n",
    "Where we multiply the embedding matrix with the weights, add the biases and collect the results. But since the embeddings are stacked ðŸ“šðŸ¥ž, the dimensions do not match. Consequently, the matrix multiplication cannot take place. To address this challenge, Andrej said we need to **concatenate** the embedding vectors. By concatenating those 2D vectors, we find ourselves with 6-D vectors that will be fed into the hidden layer. This is what we want. The remaining question is, how do you effeciently concatenate the embedding vectors. Andrej said there are many ways to do that in PyTorch because it is a large library.\n",
    "\n",
    "Using the documentation, he showed us the [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat) function which, according to the documentation, concatenates a sequence of tensors over a specified dimenstion. What Andrej did was he first _selected the embedding vector of the first character for each of the training example_, did the same thing for the second character, and the third character. By the end, he found himself with three tensors. The first with the embedding vectors of the first characters in the entire training set, the second containing the embedding vectors of the second characters in the entire training set, and the third containing the embedding vectors of the third characters in the training set. Each of the tensors is of the shape $32 \\times 2$.\n",
    "\n",
    "He finally concatenated the tensors along the dimension `1`. Which is along the rows. The first row of the three tensors are conctenanted together, the second row of the three tensors are concatenated together, up to the last row.\n",
    "\n",
    "In Python, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.2801, -0.1076],\n",
       "        [ 1.4134, -0.5302,  1.2801, -0.1076,  1.3273,  1.0913],\n",
       "        [ 1.2801, -0.1076,  1.3273,  1.0913,  1.3273,  1.0913],\n",
       "        [ 1.3273,  1.0913,  1.3273,  1.0913,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  0.4660, -0.6404],\n",
       "        [ 1.4134, -0.5302,  0.4660, -0.6404, -1.1114, -0.2912],\n",
       "        [ 0.4660, -0.6404, -1.1114, -0.2912,  0.6281,  0.2684],\n",
       "        [-1.1114, -0.2912,  0.6281,  0.2684, -1.1607, -0.7553],\n",
       "        [ 0.6281,  0.2684, -1.1607, -0.7553,  0.6281,  0.2684],\n",
       "        [-1.1607, -0.7553,  0.6281,  0.2684,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  0.9838,  0.3194, -1.1607, -0.7553],\n",
       "        [ 0.9838,  0.3194, -1.1607, -0.7553,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  0.6281,  0.2684],\n",
       "        [ 1.4134, -0.5302,  0.6281,  0.2684, -0.9109,  0.9271],\n",
       "        [ 0.6281,  0.2684, -0.9109,  0.9271,  0.9838,  0.3194],\n",
       "        [-0.9109,  0.9271,  0.9838,  0.3194,  1.2051,  1.5977],\n",
       "        [ 0.9838,  0.3194,  1.2051,  1.5977,  1.2801, -0.1076],\n",
       "        [ 1.2051,  1.5977,  1.2801, -0.1076, -1.1114, -0.2912],\n",
       "        [ 1.2801, -0.1076, -1.1114, -0.2912, -1.1114, -0.2912],\n",
       "        [-1.1114, -0.2912, -1.1114, -0.2912,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302, -0.9109,  0.9271],\n",
       "        [ 1.4134, -0.5302, -0.9109,  0.9271,  0.4660, -0.6404],\n",
       "        [-0.9109,  0.9271,  0.4660, -0.6404, -2.0544, -0.7202],\n",
       "        [ 0.4660, -0.6404, -2.0544, -0.7202,  0.1205, -0.9544],\n",
       "        [-2.0544, -0.7202,  0.1205, -0.9544,  0.6281,  0.2684],\n",
       "        [ 0.1205, -0.9544,  0.6281,  0.2684,  0.9838,  0.3194]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(\n",
    "    [embed[:, 0, :], embed[:, 1, :], embed[:, 2, :]], 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we reached the point that we wanted, Andrej said that this is an ugly solution. Why because it will not generalize if we change the `block_size` which is now $3$. If later we want to increase it, we'd have to change the code because we are indexing directly. To improve the solution, Andrej showed the `torch.unbind` function which removes a specified dimension from a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.2801, -0.1076],\n",
       "        [ 1.4134, -0.5302,  1.2801, -0.1076,  1.3273,  1.0913],\n",
       "        [ 1.2801, -0.1076,  1.3273,  1.0913,  1.3273,  1.0913],\n",
       "        [ 1.3273,  1.0913,  1.3273,  1.0913,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  0.4660, -0.6404],\n",
       "        [ 1.4134, -0.5302,  0.4660, -0.6404, -1.1114, -0.2912],\n",
       "        [ 0.4660, -0.6404, -1.1114, -0.2912,  0.6281,  0.2684],\n",
       "        [-1.1114, -0.2912,  0.6281,  0.2684, -1.1607, -0.7553],\n",
       "        [ 0.6281,  0.2684, -1.1607, -0.7553,  0.6281,  0.2684],\n",
       "        [-1.1607, -0.7553,  0.6281,  0.2684,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  0.9838,  0.3194, -1.1607, -0.7553],\n",
       "        [ 0.9838,  0.3194, -1.1607, -0.7553,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  0.6281,  0.2684],\n",
       "        [ 1.4134, -0.5302,  0.6281,  0.2684, -0.9109,  0.9271],\n",
       "        [ 0.6281,  0.2684, -0.9109,  0.9271,  0.9838,  0.3194],\n",
       "        [-0.9109,  0.9271,  0.9838,  0.3194,  1.2051,  1.5977],\n",
       "        [ 0.9838,  0.3194,  1.2051,  1.5977,  1.2801, -0.1076],\n",
       "        [ 1.2051,  1.5977,  1.2801, -0.1076, -1.1114, -0.2912],\n",
       "        [ 1.2801, -0.1076, -1.1114, -0.2912, -1.1114, -0.2912],\n",
       "        [-1.1114, -0.2912, -1.1114, -0.2912,  0.9838,  0.3194],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302,  1.4134, -0.5302],\n",
       "        [ 1.4134, -0.5302,  1.4134, -0.5302, -0.9109,  0.9271],\n",
       "        [ 1.4134, -0.5302, -0.9109,  0.9271,  0.4660, -0.6404],\n",
       "        [-0.9109,  0.9271,  0.4660, -0.6404, -2.0544, -0.7202],\n",
       "        [ 0.4660, -0.6404, -2.0544, -0.7202,  0.1205, -0.9544],\n",
       "        [-2.0544, -0.7202,  0.1205, -0.9544,  0.6281,  0.2684],\n",
       "        [ 0.1205, -0.9544,  0.6281,  0.2684,  0.9838,  0.3194]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(embed, 1), 1) #removes the first dimension, then concat along the first dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the previous solution is better because we do not do direct indexing anymore... It turns out, there is an even better and more efficient solution which also gave the opportunity to hint at the internals of `torch.tensor`. To illustrate it, Andrej proposes us to create tensor of $18$ elements like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the previously created tensor is $18$. It turns out that we can quickly represent this very same tensor, as _different size `n-dimensional` tensors_. Though it is just a row-vector, we can \"**view**\" it anything else, and the way you do this is using the `view` function. Using this function, we can say \"Oh, actually it is not single vector of $18$, but a $(2 \\times 9)$ tensor, or even $(9 \\times 2)$\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 9) #turns the vector into a (2 x 9) tensor\n",
    "#a.view(9, 2)\n",
    "#a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as multiplying the dimensions together results in the original size, it will work. And according to Andrej, the operation performed by `view` is very efficient. The reason is that _in each tensor, there is something called the underlying storage_. And the storage consist of the numbers in the tensors stored as a 1-D sequence. And this is how this vector is _represented in the computer memory_. It is always a 1-D sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/sbqpw0b93pgf3f3qn_0c6g1m0000gn/T/ipykernel_21882/214256462.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  a.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `view` function manipulates the attributes of the tensor that dictates how this 1-D sequence is interpreted (i.e. \"viewed\") as a tensor. The efficiency of `view` comes from the fact that _no memory is created, or copied_. It's just some of the internal attributes of tensor, namely the storage offset, the stride, and shape so that this 1-D sequence of bytes is \"view\" or \"seen\" as different n-dimensional array. Andrej recommended this [blog post, from Eric](http://blog.ezyang.com/2019/05/pytorch-internals/) to in further details and understanding how tensors are represented. He also said that he might create an entire video of this topic (torch internals) as well. But as of now, we keep in mind that the `view` function is an efficient function.\n",
    "\n",
    "So to back to our `embed` matrix. We know the shape is $(32 \\times 3 \\times 2)$. $32$ because we are first considering the 32 training examples, but later will consider the entire. Using the `view` function, we can ask Pytorch to \"view\" our `embed` matrix, as a $32 \\times 6$ instead. We can verify the following result by doing an element-wise equal check with of one of the previous results we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed.view(32, 6)\n",
    "#torch.equal(torch.cat(torch.unbind(embed, 1), 1) , embed.view(32, 6)) #Element-wise check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to come back to our matrix multiplication that we wanted to use to compute the activation of the hidden layer of our MLP. We wanted to do something like this:\n",
    "\n",
    "```py\n",
    "embed @ W1 + b1\n",
    "```\n",
    "\n",
    "But the dimensions were not matching up. With what we learned, we can ask Pytorch to view it differently ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1) #-1 lets PyTorch infer the right dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`h` contains the activations for all the training examples after being forward propagated through the 100 neurons in the hidden layer. Those values are $[-1, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrej brought our attention to the `+` operation that is happening as well because broadcasting happens there, so he walks us through how to make sure the broadcasting is doing what we want.\n",
    "\n",
    "`b1.shape` is $100$. We have \n",
    "\n",
    "```py\n",
    "32, 100\n",
    " 1, 100\n",
    "```\n",
    "Broadcasting aligns on the right like above, and creates a fake dimension (1) on the left... turn everything into $(1 \\times 100)$. Pythor will then copy vertically the $(1 \\times 100)$ tensor $32$ times and do an element-wise addition.\n",
    "\n",
    "It is correct because the same bias vector will be added to all of the rows containing the dot product of each training ex. with the neurons in the hidden layer. This is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the output layer\n",
    "\n",
    "Let's finally implement the output of layer of the neural network. This layer contains one neuron for each element of our vocabulary. In the case of the paper, the layer has $17,000$ neurons because it has $17,000$ possible words. In our case, we are a total of $27$ possible characters, so our layer will have $27$ neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27) #biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the logits (i.e. log-counts) which are the outputs of this of the neural network, will computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in `part-1` we want to exponentiate those log-counts, to get what Andrej calld \"fake-counts\", and normalize them into a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the negative log likelihood\n",
    "\n",
    "In this section, we get ourselves ready to implement the negative likelihood loss. We saw what it was in the part-1 of the series. In the previous section of this notebook, we computed the probabality distributions. Now, we are evaluating the quality of our model, and for that we need the correct labels. Those are stored in tensor `Y`, created when forming the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in `part-1`, we'd like to index the probability assigned to the correct label, for each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.9980e-06, 3.6825e-07, 3.0676e-07, 2.4537e-08, 1.5675e-09, 5.3992e-05,\n",
       "        1.5779e-11, 3.8735e-05, 3.2191e-07, 9.2154e-07, 5.6177e-06, 6.3665e-15,\n",
       "        3.7136e-08, 5.5197e-05, 1.4089e-03, 9.7377e-07, 1.8818e-03, 1.4742e-14,\n",
       "        4.1879e-04, 7.5153e-03, 9.1950e-01, 1.4766e-10, 3.3849e-11, 7.1389e-04,\n",
       "        5.2668e-16, 2.2177e-15, 1.5337e-10, 7.7277e-16, 3.1545e-09, 9.2787e-08,\n",
       "        4.9962e-14, 3.3620e-12])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the probabilities the network assigned to the correct label associated with the first $32$ training examples. But as we learned in `part-1`, we can multiply those probabilities together to obtain the _likelihood_, then take the `log` (which is equivalent to just adding those probabilities together). This is gives us the _log likelihood_. But since this value is negative, we negative it... to make it positive. Remember that when this value is high the network is producing close to uniform probability distributions, conversely when this value is small... it means the network is producing peaked probability distributions. In other words, it is assigning a high probability, to the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.8004)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the loss we'd like the neural net to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the full network\n",
    "\n",
    "In the previous sections, the lines of Python code defining the network are separated with explanations, which can make it difficult to follow, if one is just focused on the network. With a clear understanding of the network this time, let's bring together all the Python code defining it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2] # bring together all the parameters into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction `F.cross_entropy` and why\n",
    "\n",
    "Andrej decided to make this network \"more respectable\" he highlighed the following portion of the code:\n",
    "\n",
    "```py\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "```\n",
    "\n",
    "We are taking the logits out of the network, and calculating the loss. By using those 3 lines, he said, we are re-inventing the wheel. Since we are doing classification, which is a very common task in Machine Learning, PyTorch proposes a function that basically grab the logits out of the network, does everything we just did, and spit out a loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get the same loss value. So, we can update the previous code with the `F.cross_entropy` function like below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many good reasons to prefer `F.cross_entropy` over using our own implementation. We did it for education reasons. In practice, we do not do that. When using `F.cross_entropy`:\n",
    "\n",
    "1. PyTorch will not create intermediate tensors in memory like we did. This is inefficient to run. Pytorch instead will cluster all these operations and have fuzed kernels that evaluate these operations. This results in a \"cooler\" forward pass.\n",
    "\n",
    "2. The backward pass can be implemented more efficiently. And very often the combined expression can be simpler mathematically to backpropagate through. This results in a \"cooler\" backward pass.\n",
    "\n",
    "3. Under the hood `F.cross_entropy` is significantly well-behaved (numerically). It means that when logits take \"extreme\" values, say very positive, which can happen during optimization of the net, taking the `exp()` of such extremes result in infinite values. And trying to normalize that... well Â¯\\\\\\_(ãƒ„)\\_/Â¯. In that regards, `F.cross_entropy` is more robust than our vanilla implementation. PyTorch handles this by finding the highest number in the tensor and subtract it from each element in the tensor. This operation ensure the values are small enough to be represented as floating point values. Plus this operation does _not_ change the result of the normalization because substracting the same number everywhere maintains the relationship between number. At least, that's how I understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the training loop, overfitting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
