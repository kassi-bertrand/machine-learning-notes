{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is the second in the Makemore series. In part 1 we implemented Makemore using a bigram model. We looked at ONE previous character and produced a probability distribution of what character is likely to come next. We did that using two different approaches and reached the same results. First, we used counts and normalized them into probabilities. Second, we used a simple neural network (i.e. linear layer) to produce those same probabilities.\n",
    "\n",
    "The limit of our implementation in part 1 is that it only looks at ONE previous character; and because the bigram model took only ONE character as context, the prediction of the bigram were **not** good. If we were to use consider MORE than one character when predicting the next one, the approach we used in part-1 (storing everything in a tensor) becomes unsustainable because _the number of combinations we will have to store in a tensor grows exponentially with the amount characters we take as context_. For instance, if we were to take just three characters as context when making a prediction, we would have to store $27 \\times 27 \\times 27 = 19683$ possibilities in a tensor. That's way to many possibilities, the majority of these possibilities will have very few counts, Andrej said.\n",
    "\n",
    "That's why for this second part, we are moving away from the Bigram model. This time, we are implementing a Multi-Layer Percepton to predict the next character. The modeling approach we are going to adopt follows the paper: [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengio et al. 2003 (MLP language model) paper walkthrough\n",
    "\n",
    "This paper, Andrej said, is not the first to have proposed the use of MLPs to predict the next character/token in a sequence, but was very influencial and is very often cited. Since the paper is long ($19$ pages) Andrej decided to give us gist of it, but invited us to read the entire work. This paper is what we are going to implement in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-building our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # For making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary of characters and mapping from/to integers. It is important to recall that we are building a character-level language model, unlike the paper who is describing a word-level language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary of characters and mapping to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i + 1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to set up the dataset such that we can feed training examples to the neural network easily. We are going to refurbish code we wrote in the previous tutorial. To form the dataset in the first part, we added the first part of the bigram (the context) in a tensor, `xs`... and the second part (the correct prediction) in another tensor, `ys`. \n",
    "\n",
    "We are doing something very similar here, but this time Andrej is adding MORE than ONE character as context. `X` will serve as the input to the neural network and `Y` will serve as the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "  \n",
    "  print(w)\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    idx = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(idx)\n",
    "    print(''.join(itos[i] for i in context), '--->', itos[idx])\n",
    "    context = context[1:] + [idx] # crop and append\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
