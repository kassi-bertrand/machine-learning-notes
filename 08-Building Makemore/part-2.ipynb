{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is the second in the Makemore series. In part 1 we implemented Makemore using a bigram model. We looked at ONE previous character and produced a probability distribution of what character is likely to come next. We did that using two different approaches and reached the same results. First, we used counts and normalized them into probabilities. Second, we used a simple neural network (i.e. linear layer) to produce those same probabilities.\n",
    "\n",
    "The limit of our implementation in part 1 is that it only looks at ONE previous character; and because the bigram model took only ONE character as context, the prediction of the bigram were **not** good. If we were to use consider MORE than one character when predicting the next one, the approach we used in part-1 (storing everything in a tensor) becomes unsustainable because _the number of combinations we will have to store in a tensor grows exponentially with the amount characters we take as context_. For instance, if we were to take just three characters as context when making a prediction, we would have to store $27 \\times 27 \\times 27 = 19683$ possibilities in a tensor. That's way to many possibilities, the majority of these possibilities will have very few counts, Andrej said.\n",
    "\n",
    "That's why for this second part, we are moving away from the Bigram model. This time, we are implementing a Multi-Layer Percepton to predict the next character. The modeling approach we are going to adopt follows the paper: [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengio et al. 2003 (MLP language model) paper walkthrough\n",
    "\n",
    "This paper, Andrej said, is not the first to have proposed the use of MLPs to predict the next character/token in a sequence, but was very influencial and is very often cited. Since the paper is long ($19$ pages) Andrej decided to give us gist of it, but invited us to read the entire work. This paper is what we are going to implement in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-building our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # For making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary of characters and mapping from/to integers. It is important to recall that we are building a character-level language model, unlike the paper who is describing a word-level language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary of characters and mapping to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i + 1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to set up the dataset such that we can feed training examples to the neural network easily. We are going to refurbish code we wrote in the previous tutorial. To form the dataset in the first part, we added the first part of the bigram (the context) in a tensor, `xs`... and the second part (the correct prediction) in another tensor, `ys`. \n",
    "\n",
    "We are doing something very similar here, but this time Andrej is adding MORE than ONE character as context. `X` will store the input of the neural network and `Y` will store the correct labels. If, out of curiosity, you uncomment the commented lines, the code print all the training examples _per_ word in our dataset. For the output to be manageable, I suggest doing it for a couple words and not the entire dataset ðŸ˜‰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]: #FIVE WORDS\n",
    "  #print(w)\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    idx = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(idx)\n",
    "    #print(''.join(itos[i] for i in context), '--->', itos[idx])\n",
    "    context = context[1:] + [idx] # crop and append\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the dataset is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set `X` contains $32$ training examples of $3$ characters (i.e. context) each when **considering only $5$ words**. If we were to consider all of the words, our training set would contain $228146$ training examples, each with a context size of $3$. Also, printing the `dtype` indicates that tensors `X` and `Y` are storing `int64` values. It is because we are not storing the characters directly but rather the unique integers we assigned to each of them using `stoi` defined earlier in this notebook.\n",
    "\n",
    "Feel free to print out `X` or `Y` and see what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the embedding lookup table\n",
    "\n",
    "We have 27 possible characters and we are going to embed each of them in a lower dimensional space. In the paper, they have $17,000$ total words and embed them all into a 30 dimensional space, which Andrej said was small. Since we have just 27 characters like mentioned earlier, Andrej suggested we start with 2D embedding space. That means, each of the 27 letters will be associated with a 2D embedding vector. As a result, our embedding matrix will be of the shape $(27 \\times 2)$. In that sense, it is reasonable to look at the embedding matrix as a **lookup table**.\n",
    "\n",
    "_REMEMBER: Those embedding vectors are (initially) randomly generated._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0134,  0.6154],\n",
       "        [-0.1860,  0.1425],\n",
       "        [ 0.0377,  0.7238],\n",
       "        [-0.7667,  0.8443],\n",
       "        [ 0.1057, -0.0125],\n",
       "        [-1.1629, -0.2860],\n",
       "        [ 0.1418, -0.6362],\n",
       "        [ 0.0521,  0.4169],\n",
       "        [-1.0463,  2.0669],\n",
       "        [ 0.0645,  1.3825],\n",
       "        [-1.0832,  0.3011],\n",
       "        [ 0.6703, -0.2062],\n",
       "        [ 1.0942, -0.2610],\n",
       "        [-1.2129,  1.6612],\n",
       "        [-0.4797,  0.7566],\n",
       "        [ 0.3873,  1.3897],\n",
       "        [-0.6656,  0.2542],\n",
       "        [ 0.3217,  1.8414],\n",
       "        [-0.0679,  0.4900],\n",
       "        [-0.7947, -0.8290],\n",
       "        [ 1.1158,  0.1105],\n",
       "        [ 0.3624, -2.1147],\n",
       "        [-1.5373, -1.1123],\n",
       "        [-0.5142, -1.2584],\n",
       "        [-2.4416, -0.8317],\n",
       "        [-1.5181, -0.6986],\n",
       "        [-1.2476, -0.1986]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to index into the embedding matrix `C`. We can do it by using directly using the row index, or more interestingly, we can select specific rows of the matrix by multiplying it with a one-hot encoded vector. We illustrated this in \"NOTE 1\" section of the `part-1` notebook. For simplicity, Andrej decided to use numbers for indexing. With Pytorch, it is possible to do single-dimension indexing, meaning use a single number to select a row in a tensor or use a 1-D tensor to select multiple rows at once. Andrej showed us that is also possible to select rows in a tensor using a 2-D tensor.\n",
    "\n",
    "Using 2-D indexing, we can simultaneously retreive the embedding vector of all the integers in the training set `X` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we get a 3D matrix back? Remember each character is represented by a number in tensor `X`. For each number in `X` the corresponding embedding is returned. Since we have three numbers per row in `X`, the  is printed as a _series of $(3 \\times 2)$ matrices_. Makes sense? That's my way of looking at it.\n",
    "\n",
    "Andrej for instance said, for each of the elements in the $(32 \\times 3)$ matrix (i.e. the training set `X`), we retreived the corresponding the embedding. Makes sense?\n",
    "\n",
    "With the embedding of all the integers in `X` selected, Andrej stored them in the `embed` variable like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = C[X]\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the hidden layer + internals of torch. Tensor: storage, views\n",
    "\n",
    "In this section of the video, we implement the hidden layer of the of the neural network. Andrej decided to call the weights of this hidden layer `W1`, so we are going to do just like him.\n",
    "\n",
    "What should be the shape of `W1`? It depends on two things. (1) the number of neurons in the hidden layer. This number determines the number columns in `W1`. (2) the number of inputs per neurons. This number will determine the number of rows in `W1`.\n",
    "\n",
    "The number of inputs to the hidden layer $3 \\times 2$ which is **$6$**. Why? Because we are feeding three integers(i.e. characters) at a time and each of these integers are associated with a 2-D embedding vector. And the number of neurons in the hidden layer is a hyperparameter, meaning it is something the designer of the network has to choose. Andrej decided to go with $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100) #biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would like to do something like this:\n",
    "\n",
    "```py\n",
    "embed @ W1 + b\n",
    "```\n",
    "\n",
    "Where we multiply the embedding matrix with the weights, add the biases and collect the results. But since the embeddings are stacked ðŸ“šðŸ¥ž, the dimensions do not match. Consequently, the matrix multiplication cannot take place. To address this challenge, Andrej said we need to **concatenate** the embedding vectors. By concatenating those 2D vectors, we find ourselves with 6-D vectors that will be fed into the hidden layer. This is what we want. The remaining question is, how do you effeciently concatenate the embedding vectors. Andrej said there are many ways to do that in PyTorch because it is a large library.\n",
    "\n",
    "Using the documentation, he showed us the [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat) function which, according to the documentation, concatenates a sequence of tensors over a specified dimenstion. What Andrej did was he first _selected the embedding vector of the first character for each of the training example_, did the same thing for the second character, and the third character. By the end, he found himself with three tensors. The first with the embedding vectors of the first characters in the entire training set, the second containing the embedding vectors of the second characters in the entire training set, and the third containing the embedding vectors of the third characters in the training set. Each of the tensors is of the shape $32 \\times 2$.\n",
    "\n",
    "He finally concatenated the tensors along the dimension `1`. Which is along the rows. The first row of the three tensors are conctenanted together, the second row of the three tensors are concatenated together, up to the last row.\n",
    "\n",
    "In Python, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.1629, -0.2860],\n",
       "        [-1.0134,  0.6154, -1.1629, -0.2860, -1.2129,  1.6612],\n",
       "        [-1.1629, -0.2860, -1.2129,  1.6612, -1.2129,  1.6612],\n",
       "        [-1.2129,  1.6612, -1.2129,  1.6612, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154,  0.3873,  1.3897],\n",
       "        [-1.0134,  0.6154,  0.3873,  1.3897,  1.0942, -0.2610],\n",
       "        [ 0.3873,  1.3897,  1.0942, -0.2610,  0.0645,  1.3825],\n",
       "        [ 1.0942, -0.2610,  0.0645,  1.3825, -1.5373, -1.1123],\n",
       "        [ 0.0645,  1.3825, -1.5373, -1.1123,  0.0645,  1.3825],\n",
       "        [-1.5373, -1.1123,  0.0645,  1.3825, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -0.1860,  0.1425, -1.5373, -1.1123],\n",
       "        [-0.1860,  0.1425, -1.5373, -1.1123, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154,  0.0645,  1.3825],\n",
       "        [-1.0134,  0.6154,  0.0645,  1.3825, -0.7947, -0.8290],\n",
       "        [ 0.0645,  1.3825, -0.7947, -0.8290, -0.1860,  0.1425],\n",
       "        [-0.7947, -0.8290, -0.1860,  0.1425,  0.0377,  0.7238],\n",
       "        [-0.1860,  0.1425,  0.0377,  0.7238, -1.1629, -0.2860],\n",
       "        [ 0.0377,  0.7238, -1.1629, -0.2860,  1.0942, -0.2610],\n",
       "        [-1.1629, -0.2860,  1.0942, -0.2610,  1.0942, -0.2610],\n",
       "        [ 1.0942, -0.2610,  1.0942, -0.2610, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -0.7947, -0.8290],\n",
       "        [-1.0134,  0.6154, -0.7947, -0.8290,  0.3873,  1.3897],\n",
       "        [-0.7947, -0.8290,  0.3873,  1.3897, -0.6656,  0.2542],\n",
       "        [ 0.3873,  1.3897, -0.6656,  0.2542, -1.0463,  2.0669],\n",
       "        [-0.6656,  0.2542, -1.0463,  2.0669,  0.0645,  1.3825],\n",
       "        [-1.0463,  2.0669,  0.0645,  1.3825, -0.1860,  0.1425]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(\n",
    "    [embed[:, 0, :], embed[:, 1, :], embed[:, 2, :]], 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we reached the point that we wanted, Andrej said that this is an ugly solution. Why because it will not generalize if we change the `block_size` which is now $3$. If later we want to increase it, we'd have to change the code because we are indexing directly. To improve the solution, Andrej showed the `torch.unbind` function which removes a specified dimension from a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.1629, -0.2860],\n",
       "        [-1.0134,  0.6154, -1.1629, -0.2860, -1.2129,  1.6612],\n",
       "        [-1.1629, -0.2860, -1.2129,  1.6612, -1.2129,  1.6612],\n",
       "        [-1.2129,  1.6612, -1.2129,  1.6612, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154,  0.3873,  1.3897],\n",
       "        [-1.0134,  0.6154,  0.3873,  1.3897,  1.0942, -0.2610],\n",
       "        [ 0.3873,  1.3897,  1.0942, -0.2610,  0.0645,  1.3825],\n",
       "        [ 1.0942, -0.2610,  0.0645,  1.3825, -1.5373, -1.1123],\n",
       "        [ 0.0645,  1.3825, -1.5373, -1.1123,  0.0645,  1.3825],\n",
       "        [-1.5373, -1.1123,  0.0645,  1.3825, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -0.1860,  0.1425, -1.5373, -1.1123],\n",
       "        [-0.1860,  0.1425, -1.5373, -1.1123, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154,  0.0645,  1.3825],\n",
       "        [-1.0134,  0.6154,  0.0645,  1.3825, -0.7947, -0.8290],\n",
       "        [ 0.0645,  1.3825, -0.7947, -0.8290, -0.1860,  0.1425],\n",
       "        [-0.7947, -0.8290, -0.1860,  0.1425,  0.0377,  0.7238],\n",
       "        [-0.1860,  0.1425,  0.0377,  0.7238, -1.1629, -0.2860],\n",
       "        [ 0.0377,  0.7238, -1.1629, -0.2860,  1.0942, -0.2610],\n",
       "        [-1.1629, -0.2860,  1.0942, -0.2610,  1.0942, -0.2610],\n",
       "        [ 1.0942, -0.2610,  1.0942, -0.2610, -0.1860,  0.1425],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -1.0134,  0.6154],\n",
       "        [-1.0134,  0.6154, -1.0134,  0.6154, -0.7947, -0.8290],\n",
       "        [-1.0134,  0.6154, -0.7947, -0.8290,  0.3873,  1.3897],\n",
       "        [-0.7947, -0.8290,  0.3873,  1.3897, -0.6656,  0.2542],\n",
       "        [ 0.3873,  1.3897, -0.6656,  0.2542, -1.0463,  2.0669],\n",
       "        [-0.6656,  0.2542, -1.0463,  2.0669,  0.0645,  1.3825],\n",
       "        [-1.0463,  2.0669,  0.0645,  1.3825, -0.1860,  0.1425]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(embed, 1), 1) #removes the first dimension, then concat along the first dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the previous solution is better because we do not do direct indexing anymore... It turns out, there is an even better and more efficient solution which also gave the opportunity to hint at the internals of `torch.tensor`. To illustrate it, Andrej proposes us to create tensor of $18$ elements like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the previously created tensor is $18$. It turns out that we can quickly represent this very same tensor, as _different size `n-dimensional` tensors_. Though it is just a row-vector, we can \"**view**\" it anything else, and the way you do this is using the `view` function. Using this function, we can say \"Oh, actually it is not single vector of $18$, but a $(2 \\times 9)$ tensor, or even $(9 \\times 2)$\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 9) #turns the vector into a (2 x 9) tensor\n",
    "#a.view(9, 2)\n",
    "#a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as multiplying the dimensions together results in the original size, it will work. And according to Andrej, the operation performed by `view` is very efficient. The reason is that _in each tensor, there is something called the __underlying storage___. And this underlying storage consists of the numbers in the tensors stored as a 1-D sequence. And this is how this vector is _represented in the computer memory_. It is always a 1-D sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/sbqpw0b93pgf3f3qn_0c6g1m0000gn/T/ipykernel_38681/214256462.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  a.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `view` function manipulates the attributes of the tensor that dictate how this 1-D sequence is interpreted (i.e. \"viewed\") as a tensor. The efficiency of `view` comes from the fact that _no memory is created, or copied_. It's just some of the internal attributes of tensor, namely the storage offset, the stride, and shape so that this 1-D sequence of bytes is \"view\" or \"seen\" as different n-dimensional array. Andrej recommended this [blog post, from Eric](http://blog.ezyang.com/2019/05/pytorch-internals/) to in further details and understanding how tensors are represented. He also said that he might create an entire video of this topic (torch internals) as well. But as of now, we keep in mind that the `view` function is an efficient function.\n",
    "\n",
    "So to back to our `embed` matrix. We know the shape is $(32 \\times 3 \\times 2)$. $32$ because we are first considering the 32 training examples, but later will consider the entire. Using the `view` function, we can ask Pytorch to \"view\" our `embed` matrix, as a $32 \\times 6$ instead. We can verify the following result by doing an element-wise equal check with of one of the previous results we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed.view(32, 6)\n",
    "#torch.equal(torch.cat(torch.unbind(embed, 1), 1) , embed.view(32, 6)) #Element-wise check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to come back to our matrix multiplication that we wanted to use to compute the activation of the hidden layer of our MLP. We wanted to do something like this:\n",
    "\n",
    "```py\n",
    "embed @ W1 + b1\n",
    "```\n",
    "\n",
    "But the dimensions were not matching up. With what we learned, we can ask Pytorch to view it differently ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1) #-1 lets PyTorch infer the right dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`h` contains the activations for all the training examples after being forward propagated through the 100 neurons in the hidden layer. Those values are $[-1, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrej brought our attention to the `+` operation that is happening as well because broadcasting happens there, so he walks us through how to make sure the broadcasting is doing what we want.\n",
    "\n",
    "`b1.shape` is $100$. We have \n",
    "\n",
    "```py\n",
    "32, 100\n",
    " 1, 100\n",
    "```\n",
    "Broadcasting aligns on the right like above, and creates a fake dimension (1) on the left... turn everything into $(1 \\times 100)$. Pythor will then copy vertically the $(1 \\times 100)$ tensor $32$ times and do an element-wise addition.\n",
    "\n",
    "It is correct because the same bias vector will be added to all of the rows containing the dot product of each training ex. with the neurons in the hidden layer. This is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the output layer\n",
    "\n",
    "Let's finally implement the output of layer of the neural network. This layer contains one neuron for each element of our vocabulary. In the case of the paper, the layer has $17,000$ neurons because it has $17,000$ possible words. In our case, we are a total of $27$ possible characters, so our layer will have $27$ neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27) #biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the logits (i.e. log-counts) which are the outputs of this of the neural network, will computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in `part-1` we want to exponentiate those log-counts, to get what Andrej calld \"fake-counts\", and normalize them into a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the negative log likelihood\n",
    "\n",
    "In this section, we get ourselves ready to implement the negative likelihood loss. We saw what it was in the part-1 of the series. In the previous section of this notebook, we computed the probabality distributions. Now, we are evaluating the quality of our model, and for that we need the correct labels. Those are stored in tensor `Y`, created when forming the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in `part-1`, we'd like to index the probability assigned to the correct label, for each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4128e-06, 2.1173e-13, 4.7668e-08, 5.6790e-06, 2.7479e-08, 1.5602e-05,\n",
       "        2.4308e-11, 1.9555e-08, 2.0263e-04, 6.1093e-14, 8.1252e-01, 3.1997e-08,\n",
       "        1.8498e-02, 9.1198e-04, 2.5297e-04, 4.7415e-10, 1.3715e-05, 7.4583e-05,\n",
       "        7.9250e-08, 1.9347e-08, 7.4568e-05, 6.1387e-08, 2.2783e-08, 1.3361e-10,\n",
       "        1.1497e-02, 4.8257e-01, 8.4675e-06, 6.0632e-10, 4.2890e-10, 1.3107e-08,\n",
       "        1.4113e-04, 1.9169e-06])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the probabilities the network assigned to the correct label associated with the first $32$ training examples. But as we learned in `part-1`, we can multiply those probabilities together to obtain the _likelihood_, then take the `log` (which is equivalent to just adding those probabilities together). This is gives us the _log likelihood_. But since this value is negative, we negative it... to make it positive. Remember that when this value is high the network is producing close to uniform probability distributions, conversely when this value is small... it means the network is producing peaked probability distributions. In other words, it is assigning a high probability, to the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.3906)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the loss we'd like the neural net to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the full network\n",
    "\n",
    "In the previous sections, the lines of Python code defining the network are separated with explanations, which can make it difficult to follow, if one is just focused on the network. With a clear understanding of the network this time, let's bring together all the Python code defining it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2] # bring together all the parameters into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction `F.cross_entropy` and why\n",
    "\n",
    "Andrej decided to make this network \"more respectable\" he highlighed the following portion of the code:\n",
    "\n",
    "```py\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "```\n",
    "\n",
    "We are taking the logits out of the network, and calculating the loss. By using those 3 lines, he said, we are re-inventing the wheel. Since we are doing classification, which is a very common task in Machine Learning, PyTorch proposes a function that basically grab the logits out of the network, does everything we just did, and spit out a loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get the same loss value. So, we can update the previous code with the `F.cross_entropy` function like below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many good reasons to prefer `F.cross_entropy` over using our own implementation. We did it for education reasons. In practice, we do not do that. When using `F.cross_entropy`:\n",
    "\n",
    "1. PyTorch will not create intermediate tensors in memory like we did. This is inefficient to run. Pytorch instead will cluster all these operations and have fuzed kernels that evaluate these operations. This results in a \"cooler\" forward pass.\n",
    "\n",
    "2. The backward pass can be implemented more efficiently. And very often the combined expression can be simpler mathematically to backpropagate through. This results in a \"cooler\" backward pass.\n",
    "\n",
    "3. Under the hood `F.cross_entropy` is significantly well-behaved (numerically). It means that when logits take \"extreme\" values, say very positive, which can happen during optimization of the net, taking the `exp()` of such extremes result in infinite values. And trying to normalize that... well Â¯\\\\\\_(ãƒ„)\\_/Â¯. In that regards, `F.cross_entropy` is more robust than our vanilla implementation. PyTorch handles this by finding the highest number in the tensor and subtract it from each element in the tensor. This operation ensure the values are small enough to be represented as floating point values. Plus this operation does _not_ change the result of the normalization because substracting the same number everywhere maintains the relationship between number. At least, that's how I understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the training loop, overfitting one batch\n",
    "\n",
    "In this section of the notebook, we set up the training loop of the neural net. As we know, for each iteration of the loop we need compute forward pass, and backward pass, and update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure \"grad\" is set to True in PyTorch\n",
    "for p in parameters:\n",
    "    p.requires_grad =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2561368942260742\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    # Forward pass\n",
    "    embed = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(embed.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "        \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for $1000$ epochs (i.e. steps) gets us to a very low loss. It suggests that the model is making very good predictions, Andrej said.\n",
    "\n",
    "The reason it works so well is because is because _we are overfitting on the 32 training examples we currently have_. Remember very early in this notebook, we considered $5$ words from which we extracted $32$ training examples. Since the network has $3481$ parameters those $32$ data points are very easy to \"fit\". And that's why the network performs so seemingly well. We have so many parameters for so few examples so it's easy to lower the loss.\n",
    "\n",
    "_NOTE: Those $32$ examples are a small portion of our dataset. We refer to it as a \"**batch**\". The batch size can be anything we want._\n",
    "\n",
    "Notice also that we are not able to acheive a loss of $0$. The reason for that, Andrej said, there are example whose predictions is not obvious. To show that he first printed out the maximum logits along the first dimension and the actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([13.3347, 17.7906, 20.6013, 20.6118, 16.7355, 13.3347, 15.9986, 14.1725,\n",
       "        15.9149, 18.3614, 15.9397, 20.9265, 13.3347, 17.1088, 17.1319, 20.0600,\n",
       "        13.3347, 16.5889, 15.1016, 17.0579, 18.5863, 15.9671, 10.8740, 10.6872,\n",
       "        15.5056, 13.3347, 16.1793, 16.9743, 12.7427, 16.2007, 19.0847, 16.0194],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([19, 13, 13,  1,  0, 19, 12,  9, 22,  9,  1,  0, 19, 22,  1,  0, 19, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y #the actual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch prints out the actual max values and their index as well. Andrej said we should see that indices are close to the labels, but differ in some cases. Notably for the very first example (`...`), the predicted index is `19` but the correct label is `5`. The issue is that particular example is _associated with multiple predictions_. So, there are multiple outcomes for the single training example `...`. So that's why, Andrej said, we are not able to perfectly \"overfit\" the 32 examples, and make the loss exactly to zero. In the case, where the unique input is associated with a unique output, the model memorized those thus got the right result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the full dataset, minibatches\n",
    "\n",
    "The simple thing to do to not overfit a few examples is to make sure we are reading the entire dataset of words... which is not the case right now. To prevent you from scrolling back and forth, I rebuild a new dataset and feed it through the network. Since we are using the same seed, the same weights will be generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMING THE ENTIRE DATASET\n",
    "block_size = 3 \n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "  #print(w)\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    idx = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(idx)\n",
    "    #print(''.join(itos[i] for i in context), '--->', itos[idx])\n",
    "    context = context[1:] + [idx] # crop and append\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2] # bring together all the parameters into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NUMBER OF PARAMETERS\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure \"grad\" is set to True in PyTorch\n",
    "for p in parameters:\n",
    "    p.requires_grad =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.505229949951172\n",
      "17.08449363708496\n",
      "15.776532173156738\n",
      "14.83333683013916\n",
      "14.002594947814941\n",
      "13.253252029418945\n",
      "12.579914093017578\n",
      "11.983097076416016\n",
      "11.470491409301758\n",
      "11.051855087280273\n"
     ]
    }
   ],
   "source": [
    "# THE TRAINING LOOP - OLD - BASE\n",
    "for _ in range(10):\n",
    "    # Forward pass\n",
    "    embed = C[X]\n",
    "    h = torch.tanh(embed.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we could observe the loss coming down every iteration, Andrej pointed out the value of the loss were being printed out slowly. I noticed it to lesser extent on my machine probably because my computer is faster than his. But I got the point. The reason for this behavior, he said, was that **we are doing way too much work forwarding and backwarding $228146$ examples**. In practice, people perform forwarding and backwarding on **minibatches (i.e. batches)** of the data.\n",
    "\n",
    "So, what we want to do instead is randomly select some portion of the dataset, do a forward and backward passes on that little minibatch and repeat the process. Do that, we have to modify the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5121800899505615\n"
     ]
    }
   ],
   "source": [
    "# THE TRAINING LOOP - OLD - IMPLEMENTING MINIBATCHES\n",
    "for _ in range(1000):\n",
    "    # Minibatch construct\n",
    "    idx = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    embed = C[X[idx]] # (32, 3, 2)\n",
    "    h = torch.tanh(embed.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[idx])\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are doing \"minibatching\" now, the quality of the computed gradient is lower. The direction is not as reliable as the _exact_ gradient computed over the entire dataset. When the exact gradient will go straight, a minibatch gradient will zigzag; the minibatch gradient is an approximation. But this zig zag is good enough that is it used in practice.\n",
    "\n",
    "Andrej said: \"**It is much better to have an approximate gradient and make more steps (i.e. train longer), than it is to evaluate the exact gradient and take fewer steps**\".\n",
    "\n",
    "After the training the model for $1000$ or more. Andrej proposed to see the loss over the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6582, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = C[X]\n",
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the reach the current loss I, like Andrej, ran the `training loop` $6$ times. In other words, I trained the model over $6000$ epochs. Andrej said that one issue is that we do not know if we training the model \"too slow\" or \"too fast\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding a good learning rate\n",
    "\n",
    "In this section, Andrej wants to us how to find a good learning rate and how to make sure and gain confidence we are stepping at the right speed. \n",
    "\n",
    "We started by:\n",
    "\n",
    "1. Resetting our parameters (i.e weights)\n",
    "\n",
    "2. Find a reasonable search range for the learning rate. Instead of running the training loop for `1000` steps, he ran it for `100`.\n",
    "\n",
    "    - He tried `-0.001`, and noticed the loss bearly decreased indicating the model was learning \"too slow\". He tried `-0.001`; the loss decreased faster so Andrej chose it as an adequate lower range.\n",
    "    \n",
    "    - He tried to find the learning rate for which the loss \"explodes\". He tried `1` the loss decreased fast, but was unstable. Meaning it went up and down. So, he called `-1` a fast learning rate. He tried `-10` and the loss was BIG with not signs of descrease, lol. `-10` is too big he said.\n",
    "    \n",
    "With this guess work, Andrej concluded that the right learning rate for our network is between $-0.001$ and $-1$. The way Andrej tried different learning rates efficiently in this range was by first generate a list of candidates using `torch.linespace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_exp = torch.linspace(-3, 0, 1000) # A list of 1000 exponents\n",
    "lr_space = 10**lr_exp # actual learning rate space we are going to search over.\n",
    "#lr_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those learning rates are spaced exponentially in this interval, and these are the learning rates we want to search over. Notice they are between `0.001` and `1`. So, now we are going to update the training loop once again to do that.\n",
    "\n",
    "_NOTE: Before running the following training loop, Reset the weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad =  True\n",
    "\n",
    "# THE TRAINING LOOP - NEW - TRYING MULTIPLE LEARNING RATE\n",
    "\n",
    "lr_i = [] # Learning rate for ith iteration\n",
    "loss_i = [] # Loss for the ith iteration\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    # Minibatch construct\n",
    "    idx = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    embed = C[X[idx]] # (32, 3, 2)\n",
    "    h = torch.tanh(embed.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[idx])\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    lr = lr_space[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # Track stats\n",
    "    lr_i.append(lr_exp[i]) #log the exponent of the learning rate.\n",
    "    loss_i.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. Remember, for every iteration of the loop, we tried a different learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x128537520>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_i, loss_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One the `x`-axis we have the exponent of the learning rate.\n",
    "\n",
    "We can see that the exponent of the learning rate that is good to use is roughly in the $[-1, -0.5]$ interval. Before that learning rate is too low, and after that the loss starts to explode. Andrej concluded that $-0.1$ is fairly a good exponent for the learning rate and $10^{-0.1} = 0.1$. Which means the initial guess of the $0.1$ was pretty good already. But this time, we know how to determine a good learning rate.\n",
    "\n",
    "With this knowledge, we can once again update the training loop to reflect this, and this time increase the number of steps with the confidence that the learning rate is actually good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3168, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE TRAINING LOOP\n",
    "for i in range(10000):\n",
    "    \n",
    "    # Minibatch construct\n",
    "    idx = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    embed = C[X[idx]] # (32, 3, 2)\n",
    "    h = torch.tanh(embed.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[idx])\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "# LOSS OVER ENTIRE DATASET\n",
    "embed = C[X]\n",
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this model more than one gets to a loss that is better (i.e. lower) than the one bigram's which to recall was standing to $2.45$~$7$.\n",
    "\n",
    "Andrej proposed to implement learning rate decay. Though this looks janky and is not how things happen in production, Andrej said, we first find a decent learning rate using the approach he showed us, then train the model for awhile. When the model is not learning very much and start to plateau, please like to decrease (i.e. decay) the loss by a factor of 10 and train the model for few more steps.\n",
    "\n",
    "In this notebook, we started with `0.1` trained it for $30,000$ steps. Then decayed the learning rate to `0.01` and trained the network for an additional $40,000$ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting up the dataset into train/val/test splits and why\n",
    "\n",
    "We are acheiving a lower loss compared to the bigram model. But that is not exactly true, because the model is fairly small considering the number of parameters. The paramaters of this model will grow as we add more neurons to it. But as the network's parameters grow, so is its capacity to overfitting the training set. The result of that, will be a loss that gets close to almost zero like we saw before. And that's because the network is memorizing the examples of the training set, and sampling from it will just return examples from the training set. You want get any new data. In those circumstances, this is undesirable.\n",
    "\n",
    "The standard in the field, Andrej said, is to split your data into three parts:\n",
    "\n",
    "1. `training` (80%): Used to optimize the parameters of the neural network.\n",
    "\n",
    "2. `dev` or `validation` (10)%: Use for development of hyperparameters such as the size of the hidden layer, the size of the size of the embedding, the strength of the regularization, etc. You can try different variations of the neural network and see which configuration of hyperparameters works best.\n",
    "\n",
    "3. `test` (10%): Used to evaluate of the performance of the model at the end. We evaluate on this split very sparingly, and very few times because you'd start training also on this split. It exposes us to overfitting on this split as well, which is undesirable for our purposes.\n",
    "\n",
    "Let's follow Andrej and split our datasets in those three parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "def build_dataset(words):  \n",
    "    block_size = 3 # context length\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "          idx = stoi[ch]\n",
    "          X.append(context)\n",
    "          Y.append(idx)\n",
    "          #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "          context = context[1:] + [idx] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "X_train, Y_train = build_dataset(words[:n1])\n",
    "X_dev, Y_dev = build_dataset(words[n1:n2])\n",
    "X_test, Y_test = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reset the network, and also update the training loop to use the training set and not the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE TRAINING LOOP\n",
    "for i in range(10000):\n",
    "    \n",
    "    # Minibatch construct\n",
    "    idx = torch.randint(0, X_train.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    embed = C[X_train[idx]] # (32, 3, 2)\n",
    "    h = torch.tanh(embed.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y_train[idx])\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3269, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOSS OVER DEV SET\n",
    "embed = C[X_test]\n",
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y_test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I evaluated the loss over the dev after training the network (over the training set ) for $30,000$ steps with `0.1` learning, then for $10,000$ steps with `0.01` decayed learning rate, and was able to acheive a loss of `2.3` on the development set.\n",
    "\n",
    "The examples in the dev set have never been seen by the network and yet it performed fairly well on this judjing from the loss, Andrej said. He suggested we also look at the loss over the training set as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3296, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOSS OVER TRAINING SET\n",
    "embed = C[X_train]\n",
    "h = torch.tanh(embed.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss are similar indicating we are not overfitting, according to Andrej. He continued and also said the model does not have enough parameters to memorize the entire training set. So, far we are **underfitting**. Why? because the training loss and dev loss are roughly the **same**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
