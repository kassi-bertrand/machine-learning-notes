{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9785ca99",
   "metadata": {},
   "source": [
    "# Makemore - Part 1\n",
    "\n",
    "Like previously mentioned, Makemore makes \"more\" of things you provide to it. Simple as that. This Jupyter notebook, is my first step in the journey in building **Makemore**. Let's do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3bc090",
   "metadata": {},
   "source": [
    "Under the hood, Andrej said, Makemore is a **character-level language model**. It means that Makemore will model sequences of characters. In order words, Makemore tries to predict the next character, based on previous characters. Another way to put this, would be to say that Makemore tries to answer the following question:\n",
    "\n",
    "> Based on the previous characters, what character is likely to come **next**?\n",
    "\n",
    "To provide contrast, ChatGPT is a *token-level language model*. It attempts to predict the next token (i.e. words) based on the previous tokens.\n",
    "\n",
    "Without further talking, let's start the building with loading the dataset `names.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc3448",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In this section of the notebook, I load in the dataset contained in `names.txt` in a string, split it to get individual words, then insert them in a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7c3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef4478",
   "metadata": {},
   "source": [
    "And we can go ahead and display the first 10 element of the list. Just to see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf74047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf9262",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "We would like to learn more about this dataset, so let's go ahead and print out the total number of words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00910974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097c110",
   "metadata": {},
   "source": [
    "Let's print out the shortest and longuest words, in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c867631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e5fbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ea9cd",
   "metadata": {},
   "source": [
    "Let's think through our character-level language model for a bit. Remember, its job is to predict the **next character**, given some already concrete sequence of characters before it. So, the existence of a single word in the dataset like `isabella`, Andrej said, tells us that:\n",
    "\n",
    "- The character `i` is very likely to come first in a name ü§î\n",
    "\n",
    "- The character `s` is likely to follow the character `i`\n",
    "\n",
    "- The character `a` is likely to follow the sequence `is`\n",
    "\n",
    "- The character `b` is likely to follow the sequence `isa`\n",
    "\n",
    "- ... And so on\n",
    "\n",
    "- There is also one last bit of information in the `isabella` word. It is that after all those letters have been predicted, the word is likely to be **finished** ü§∑üèæ‚Äç‚ôÇÔ∏è.\n",
    "\n",
    "This is an example of information in terms of statistical structure of what is likely to follow that can be extracted from the character-sequence, `isabella`. And isabella is not our only example! We have 32,000 of them üòé.\n",
    "\n",
    "So, our goal writing this program is capture the statistical structure in those 32,000 training examples.\n",
    "\n",
    "And in this notebook, we are going to implement a **bigram** model to acheive the previously mentioned goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703dfeb",
   "metadata": {},
   "source": [
    "## A Bigram model\n",
    "\n",
    "See, in a Bigram model we are only looking at **two characters at a time**. That's it, just ‚úåüèæ. It means that given a character... we are just trying to predict the *next character* in the sequence.\n",
    "\n",
    "The Bigram model we are set to build is concerned with modeling the \"local relationship (or structure)\" between pairs of characters. The downside is that *it only looks at 2 characters*. It ignores anything before the current character.\n",
    "\n",
    "For this reason, Bigram models are simple, but weak. But Andrej said, they are a great place to start. I believe him. ü´°.\n",
    "\n",
    "Let's continue and explore the bigrams in the dataset and see what they look like. And these Bigrams are just characters in a row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1396582",
   "metadata": {},
   "source": [
    "## Exploring the Bigrams in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e3b5d",
   "metadata": {},
   "source": [
    "To print a bigram, we must iterate on each words... and print two characters at a time. A bit like a sliding window. So, let's do that with the first word in our dataset, `emma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d9b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "for w in words[:1]:\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66426bfa",
   "metadata": {},
   "source": [
    "These are the consecutive characters in the first word. But like Andrej, we have to be careful... because we have more information than those three pairs of characters.\n",
    "\n",
    "If you remember, with our example `isabella`... the first information we got out it was that the character `i` was likely to come first, and the last information was the character `a` was likely to mark the end of the word. \n",
    "\n",
    "To be able to capture those two pieces of information, we can imagine that every example (i.e. names) in our dataset begins with a special \"start\" character (`<S>`) and ends with a special \"end\" character (`<E>`).\n",
    "\n",
    "With that in mind, let's modify our little snippet of code above to reflect the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d35328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a <E>\n"
     ]
    }
   ],
   "source": [
    "for w in words[:1]:\n",
    "    chs = ['<S>'] + list(w) + ['<E>'] #print 'chs' to see what is it now\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a5072",
   "metadata": {},
   "source": [
    "`(\"<S>\", \"e\")` is a bigram of the special \"start\" character and the letter \"e\".\n",
    "    \n",
    "`(\"a\", \"<E>\")` is a bigram of the letter \"a\" and the special \"end\" character.\n",
    "\n",
    "But what does that mean? For the first bigram... it means that given the special \"start\" character, the letter that is likely to come next is \"e\". Whereas for the second bigram... it means that the letter \"e\" is likely to be the last letter in the word.\n",
    "\n",
    "Let's print the bigram of the first three words and see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be6a96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a <E>\n",
      "<S> o\n",
      "o l\n",
      "l i\n",
      "i v\n",
      "v i\n",
      "i a\n",
      "a <E>\n",
      "<S> a\n",
      "a v\n",
      "v a\n",
      "a <E>\n"
     ]
    }
   ],
   "source": [
    "for w in words[:3]:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ebff1",
   "metadata": {},
   "source": [
    "## Counting the Bigrams in a Python Dictionary\n",
    "\n",
    "In order to find out the statistics of which character is likely to follow another, the simple way is to count. In this section of the report, we are counting how often each of those character pairs occurs in the dataset.\n",
    "\n",
    "For that we need a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9ddc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {} #bigram -> count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead828a",
   "metadata": {},
   "source": [
    "This dictionary maps a given bigram to a number. What is going to happen is that for each bigram we encounter, we will increment the count. Let's implement it in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ddde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        count_dict[bigram] = count_dict.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6a342",
   "metadata": {},
   "source": [
    "Before moving forward, I would like to explain the following syntax\n",
    "\n",
    "```py\n",
    "count_dict.get(bigram, 0) + 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05b8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
