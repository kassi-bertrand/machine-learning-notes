{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9785ca99",
   "metadata": {},
   "source": [
    "# Makemore - Part 1\n",
    "\n",
    "Like previously mentioned, Makemore makes \"more\" of things you provide to it. Simple as that. This Jupyter notebook, is my first step in the journey in building **Makemore**. Let's do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3bc090",
   "metadata": {},
   "source": [
    "Under the hood, Andrej said, Makemore is a **character-level language model**. It means that Makemore will model sequences of characters. In order words, Makemore tries to predict the next character, based on previous characters. Another way to put this, would be to say that Makemore tries to answer the following question:\n",
    "\n",
    "> Based on the previous characters, what character is likely to come **next**?\n",
    "\n",
    "To provide contrast, ChatGPT is a *token-level language model*. It attempts to predict the next token (i.e. words) based on the previous tokens.\n",
    "\n",
    "Without further talking, let's start the building with loading the dataset `names.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc3448",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In this section of the notebook, I load in the dataset contained in `names.txt` in a string, split it to get individual words, then insert them in a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7c3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef4478",
   "metadata": {},
   "source": [
    "And we can go ahead and display the first 10 element of the list. Just to see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf74047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf9262",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "We would like to learn more about this dataset, so let's go ahead and print out the total number of words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00910974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097c110",
   "metadata": {},
   "source": [
    "Let's print out the shortest and longuest words, in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c867631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e5fbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ea9cd",
   "metadata": {},
   "source": [
    "Let's think through our character-level language model for a bit. Remember, its job is to predict the **next character**, given some already concrete sequence of characters before it. So, the existence of a single word in the dataset like `isabella`, Andrej said, tells us that:\n",
    "\n",
    "- The character `i` is very likely to come first in a name ü§î\n",
    "\n",
    "- The character `s` is likely to follow the character `i`\n",
    "\n",
    "- The character `a` is likely to follow the sequence `is`\n",
    "\n",
    "- The character `b` is likely to follow the sequence `isa`\n",
    "\n",
    "... And so on\n",
    "\n",
    "- There is also one last bit of information in the `isabella` word. It is that after all those letters have been predicted, the word is likely to be **finished** ü§∑üèæ‚Äç‚ôÇÔ∏è.\n",
    "\n",
    "This is an example of information in terms of statistical structure of what is likely to follow that can be extracted from the character-sequence, `isabella`. And isabella is not our only example! We have 32,000 of them üòé.\n",
    "\n",
    "So, our goal writing this program is capture the statistical structure in those 32,000 training examples.\n",
    "\n",
    "And in this notebook, we are going to implement a **bigram** model to acheive the previously mentioned goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703dfeb",
   "metadata": {},
   "source": [
    "## A Bigram model\n",
    "\n",
    "See, in a Bigram model we are only looking at **two characters at a time**. That's it, just ‚úåüèæ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
