{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0443dd81",
   "metadata": {},
   "source": [
    "# Baby steps with Scikit-learn\n",
    "\n",
    "In this notebook, we will try to train a perceptron using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746377f",
   "metadata": {},
   "source": [
    "Let's start by loading the Iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb565716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(type(iris.data))\n",
    "print(type(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd569f1b",
   "metadata": {},
   "source": [
    "The Iris data set was loaded and stored in the `iris` object. This `iris` object is of type `Bunch` (a dictionary on steroids ðŸ’‰). A function or method can output such objects to return _multiple values_ accessible **by key**.\n",
    "\n",
    "So, the `Bunch` object returned by function `load_iris` has a key called `data` associated with the *features set*, and another one called `target` associated with the _labels_. Notice, both values associated with those keys are Numpy arrays.\n",
    "\n",
    "We'll thus use both `iris.data` and `iris.target` to manipulate our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70227302",
   "metadata": {},
   "source": [
    "But remember, we will be using only two features of the dataset for visualization purpose; exactly like we did in the previous chapter. As a recall, the two features we will use are the **petal length** and the **petal width**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5121b",
   "metadata": {},
   "source": [
    "Let's form our training set :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8f3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels:  [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "X = iris.data[:, [2, 3]] # Select all lines, but only the 3rd and 4th column of the dataset\n",
    "y = iris.target #grab the label column\n",
    "\n",
    "print(\"Class labels: \", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba1c4c",
   "metadata": {},
   "source": [
    "The training set was successfully formed, and the `np.unique(y)` returned the three unique class labels of our training set; where `Iris-setosa = 0`, `Iris-versicolor = 1`, and `Iris-virginica = 2`.\n",
    "\n",
    "Scikit-learn conviniently assigned numbers to our labels. Remember, it was task we had to do in the previous chapter. Is that nice? ðŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb692250",
   "metadata": {},
   "source": [
    "Now, let's split our training set.\n",
    "\n",
    "In Machine Learning, it is common to **cut** the training set into **two parts**. One for **training** the model, and the other to **evaluate** it. So, far we used the entire dataset to train our models. In Chapter 4, we should discuss this practice in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c43320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e6446",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "We asked `train_test_split` to randomly split `X` and `y` into 30% test data and 70% training data. The `stratify` option helps us make sure that the training and test subsets have the same proportion of class label as the input dataset. We can verifiy that using Numpy `bincount` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e09b3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in y: [50 50 50]\n",
      "Labels counts in y_train: [35 35 35]\n",
      "Labels counts in y_test: [15 15 15]\n"
     ]
    }
   ],
   "source": [
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a94d8",
   "metadata": {},
   "source": [
    "Now, let's standardize the features of our taining set. Remember, standardizing (i.e. feature scaling) helps bringing different variables under the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9c4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train) # determines the sample mean and std deviation for each feature\n",
    "\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da13af",
   "metadata": {},
   "source": [
    "Let's now train a perceptron model using our standardized training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f80ba28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(eta0=0.1, random_state=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perceptron = Perceptron(eta0=0.1, random_state=1)\n",
    "perceptron.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982bb33",
   "metadata": {},
   "source": [
    "Let's test our perceptron and print out the number of examples it misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542ff66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 1\n"
     ]
    }
   ],
   "source": [
    "y_pred = perceptron.predict(X_test_std)\n",
    "print('Misclassified examples: %d' %(y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cce020",
   "metadata": {},
   "source": [
    "We can choose to look at the number of misclassification or the accuracy to judge the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf7adb",
   "metadata": {},
   "source": [
    "Our perceptron misclassified 1 out of the 45 flowers examples in the test subset. The misclassification is then `~0.022 (1 / 45)` or `2.2 %`. It implies that our perceptron accuracy is `1 - 0.022 = 0.978` or  `97.8%`. But we can obtain this metric also in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14322c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracry: 0.978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracry: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09a77b",
   "metadata": {},
   "source": [
    "Remember that the perceptron performs best when the classes are linearly separable. If we were to plot the decision regions (which I encourage you to do), we'd see that some of the class cannot be _perfectly_ seperated linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e6649",
   "metadata": {},
   "source": [
    "**Hint**: See the book, to find the `plot_decision_regions` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eea89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
