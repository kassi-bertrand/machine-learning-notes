{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e4a0a6",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks - Sentiment Analysis\n",
    "\n",
    "In this notebook, my goal is to implement an RNN to analyze the expressed opinion of a sentence and classifies it as positive or negative using PyTorch.\n",
    "\n",
    "I will implement a multilayer RNN for sentiment analysis using a many-to-one architecture.\n",
    "\n",
    "Okay, let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7afd20",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing the data\n",
    "\n",
    "For this exercice, I will the IMDb movie reviews. The multilayer RNN to be implemented will analyze those reviews and classify them as a positive (1) or a negative (0) review.\n",
    "\n",
    "Thankfully, the IMDb movie reviews dataset is available in PyTorch through the `torchtext.datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fa2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchtext\n",
    "#pip install 'portalocker>=2.0.0'\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "test_iter = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564bb23",
   "metadata": {},
   "source": [
    "The previous two lines return iterators, specifically `ShardingFilterIterDataPipe` object. As I am writing this, the PyTorch documentation says:\n",
    "\n",
    "> The datasets supported by torchtext are datapipes from the torchdata project, which is still in Beta status. This means that the API is subject to change without deprecation cycles. In particular, we expect a lot of the current idioms to change with the eventual release of `DataLoaderV2` from `torchdata`.\n",
    "\n",
    "In case something changes that causes the code in this notebook to break, please let me know and I will update it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a9116",
   "metadata": {},
   "source": [
    "With that out of the way, I would like to print the first training example, and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b99f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_iter)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a068c",
   "metadata": {},
   "source": [
    "`train_iter` is an iterator that lets us traverse a list of tuples. Where _each_ tuple is the sentiment, followed by the text.\n",
    "This is the very first training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf7b2f",
   "metadata": {},
   "source": [
    "Like before, I should prepare the dataset before using it. \n",
    "\n",
    "I start with splitting the training partition of the dataset into a training and validation partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c02cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "torch.manual_seed(1)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    list(train_iter), [20000, 5000]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9ee9c10",
   "metadata": {},
   "source": [
    "My second step is to **find the number of unique tokens (words)** in the training dataset. It is also known as the **vocab size**.\n",
    "\n",
    "To help us acheive this goal, I use the `tokenizer` helper that I will write in the following cells. Its job is to remove punctuation, HTML markups, and other non-letter characters from each reviews in the dataset. Let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da82dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenizer(text):\n",
    "    #Remove HTML tags in \"text\".\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    #find all occurrences of emoticons in \"text.lower\", place them in a list\n",
    "    emoticons = re.findall(\n",
    "        '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n",
    "    )\n",
    "    \n",
    "    #Substitute non-word characters (equivalent to [^a-zA-Z0-9_]) in \"text.lower\" with ' '\n",
    "    #Join in ONE string elements in the \"emoticons\" list separated by a space, Then replace all '-' by empty string.\n",
    "    #Concatenate.\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    \n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3e4ce88",
   "metadata": {},
   "source": [
    "In case you still have a hard time understanding what the regular expressions do despite the comments, I suggest you go to the [Regex101](https://regex101.com/) website, paste a test string and see what each individual regular expressions in the `tokenizer` function matches. I used the training example of earlier as my test string for instance. I added some emoticons like \":-)\" and \":-(\" and played around."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8237c29f",
   "metadata": {},
   "source": [
    "With our `tokenizer` helper written, let's determine the vocab size of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be03c1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset vocab size: 69023\n"
     ]
    }
   ],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print('Training dataset vocab size:', len(token_counts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ef02530",
   "metadata": {},
   "source": [
    "Nice! Let's continue. Now, for my third step I want to **assign a unique integer to each individual token** we detected in the previous step. You may think of this unique integer as the \"id\" of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0db17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    token_counts.items(), key=lambda x: x[1], reverse=True\n",
    ")\n",
    "\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "#vocab object maps tokens to indices\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "vocab.insert_token(\"<pad>\", 0) #padding token to adjust sequence length\n",
    "vocab.insert_token(\"<unk>\", 1) #unknown tokens are assigned integer 1\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "219f521e",
   "metadata": {},
   "source": [
    "`token_counts.items()` returns a list of tuples where each tuple is a (token, frequency) pair. The `key` parameter is a function to specify how two elements from the list should be compared. In this case, we want compare tuples by looking at the second item in them (i.e. the frequency). Lastly, the `reversed` parameter specified that we want the elements in the list to be sorted in descending order. Using the `sorted` function, we are sorting the tokens in the vocab from the most common, to the least common.\n",
    "\n",
    "Then, we use the sorted list to create an `OrderedDict`. An \"ordered\" dictionary is a dictionary that preserves the order of key-value pairs. Updading the value of existing key, then the order remains unchanged. If you remove an item and reinsert it, then the item is added at the end of the dictionary.\n",
    "\n",
    "And then the `vocab` object is creates which assigns indices to the tokens in the ordered dictionary. Let's print some integer values associated with some english words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13cfaff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 7, 35, 457]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[token] for token in ['this', 'is', 'an', 'example'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5181bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
