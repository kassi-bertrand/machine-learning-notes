{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e4a0a6",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks - Sentiment Analysis\n",
    "\n",
    "In this notebook, my goal is to implement an RNN to analyze the expressed opinion of a sentence and classifies it as positive or negative using PyTorch.\n",
    "\n",
    "I will implement a multilayer RNN for sentiment analysis using a many-to-one architecture.\n",
    "\n",
    "Okay, let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7afd20",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing the data\n",
    "\n",
    "For this exercice, I will the IMDb movie reviews. The multilayer RNN to be implemented will analyze those reviews and classify them as a positive (1) or a negative (0) review.\n",
    "\n",
    "Thankfully, the IMDb movie reviews dataset is available in PyTorch through the `torchtext.datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fa2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchtext\n",
    "#pip install 'portalocker>=2.0.0'\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564bb23",
   "metadata": {},
   "source": [
    "The previous two lines return iterators, specifically `ShardingFilterIterDataPipe` object. As I am writing this, the PyTorch documentation says:\n",
    "\n",
    "> The datasets supported by torchtext are datapipes from the torchdata project, which is still in Beta status. This means that the API is subject to change without deprecation cycles. In particular, we expect a lot of the current idioms to change with the eventual release of `DataLoaderV2` from `torchdata`.\n",
    "\n",
    "In case something changes that causes the code in this notebook to break, please let me know and I will update it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a9116",
   "metadata": {},
   "source": [
    "With that out of the way, I would like to print the first training example, and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b99f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca4a068c",
   "metadata": {},
   "source": [
    "`train_dataset` is an iterator that lets us traverse a list of tuples. Where _each_ tuple is the sentiment, followed by the text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0cf7b2f",
   "metadata": {},
   "source": [
    "Let's start our preprocessing with splitting the training partition of the dataset into a training and validation partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c02cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import random_split\n",
    "torch.manual_seed(1)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    list(train_dataset), [20000, 5000]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9ee9c10",
   "metadata": {},
   "source": [
    "My second step is to **find the number of unique tokens (words)** in the training dataset. It is also known as the **vocab size**.\n",
    "\n",
    "To help us acheive this goal, I use the `tokenizer` helper that I will write in the following cells. Its job is to remove punctuation, HTML markups, and other non-letter characters from each reviews in the dataset. Let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da82dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenizer(text):\n",
    "    #Remove HTML tags in \"text\".\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    #find all occurrences of emoticons in \"text.lower\", place them in a list\n",
    "    emoticons = re.findall(\n",
    "        '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n",
    "    )\n",
    "    \n",
    "    #Substitute non-word characters (equivalent to [^a-zA-Z0-9_]) in \"text.lower\" with ' '\n",
    "    #Join in ONE string elements in the \"emoticons\" list separated by a space, Then replace all '-' by empty string.\n",
    "    #Concatenate.\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    \n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3e4ce88",
   "metadata": {},
   "source": [
    "In case you still have a hard time understanding what the regular expressions do despite the comments, I suggest you go to the [Regex101](https://regex101.com/) website, paste a test string and see what each individual regular expressions in the `tokenizer` function matches. I used the training example of earlier as my test string for instance. I added some emoticons like \":-)\" and \":-(\" and played around."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8237c29f",
   "metadata": {},
   "source": [
    "With our `tokenizer` helper written, let's determine the vocab size of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be03c1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset vocab size: 69023\n"
     ]
    }
   ],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print('Training dataset vocab size:', len(token_counts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ef02530",
   "metadata": {},
   "source": [
    "Nice! Let's continue. Now, for my third step I want to **assign a unique integer to each individual token** we detected in the previous step. You may think of this unique integer as the \"id\" of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0db17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    token_counts.items(), key=lambda x: x[1], reverse=True\n",
    ")\n",
    "\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "#vocab object maps tokens to indices\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "vocab.insert_token(\"<pad>\", 0) #padding token to adjust sequence length\n",
    "vocab.insert_token(\"<unk>\", 1) #unknown tokens are assigned integer 1\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "219f521e",
   "metadata": {},
   "source": [
    "`token_counts.items()` returns a list of tuples where each tuple is a (token, frequency) pair. The `key` parameter is a function to specify how two elements from the list should be compared. In this case, we want compare tuples by looking at the second item in them (i.e. the frequency). Lastly, the `reversed` parameter specified that we want the elements in the list to be sorted in descending order. Using the `sorted` function, we are sorting the tokens in the vocab from the most common, to the least common.\n",
    "\n",
    "Then, we use the sorted list to create an `OrderedDict`. An \"ordered\" dictionary is a dictionary that preserves the order of key-value pairs. Updading the value of existing key, then the order remains unchanged. If you remove an item and reinsert it, then the item is added at the end of the dictionary.\n",
    "\n",
    "And then the `vocab` object is created which assigns integer values to the tokens in the ordered dictionary. Let's print some integer values associated with some of words found in the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13cfaff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 7, 35, 457]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[token] for token in ['this', 'is', 'an', 'example'] ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c5181bd",
   "metadata": {},
   "source": [
    "Our tokens have been assigned unique integer values. Our next step is to generate batches of example using `DataLoader`.\n",
    "\n",
    "I start with writing two helper functions: `text_pipeline` and `label_pipeline`. `text_pipeline` expects a string as parameter and return an array of integer values of EACH token in the parameter. `label_pipeline` on the other hand, accepts a number (1 or 2), where 1 means negative and 2 means positive. `label_pipeline` returns 1 if label is 2(positive) or 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed9cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda text: [vocab[token] for token in tokenizer(text)]\n",
    "label_pipeline = lambda label: 1 if label == 2 else 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbc76cc1",
   "metadata": {},
   "source": [
    "I continue with the `collate_batch` function. This function, implement in the next Python cell, is meant to be passed as the `collate_fn` argument of the `DataLoader` constructor. The `collate_fn` argument of the `DataLoader` constructor is a function used to process the list of samples from a batch. As a result, the `collate_batch` function accepts a `batch` parameter which is a list with _all_ the samples in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309ed4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        \n",
    "        processed_text = torch.tensor(text_pipeline(_text),\n",
    "                                      dtype=torch.int64)\n",
    "        \n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0)) #processed_text is a 1-D tensor\n",
    "    \n",
    "    label_list = torch.Tensor(label_list)\n",
    "    lengths = torch.Tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True\n",
    "    )\n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb6f58c2",
   "metadata": {},
   "source": [
    "Let's to generate small batch of 4 training examples, and use our `collate_batch` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc0ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0048d913",
   "metadata": {},
   "source": [
    "We created the dataloader, and if I go ahead and print out the shape of the tensors returned by the `collate_batch` function, we'll see that the function was indeed used to process the samples in the batch. Let's go ahead and do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a03251e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,\n",
      "             4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,\n",
      "           100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,\n",
      "             5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,\n",
      "         11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,\n",
      "          1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,\n",
      "         42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,\n",
      "           148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,\n",
      "          1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,\n",
      "         15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,\n",
      "          3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,\n",
      "            27,   712,    16,     2,   220,    17,     4,    54,   722,   238,\n",
      "           395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,\n",
      "          5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,\n",
      "           155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,\n",
      "           390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,\n",
      "            31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  216,   175,   724,     5,    11,    18,    10,   226,   110,    14,\n",
      "           182,    78,     8,    13,    24,   182,    78,     8,    13,   166,\n",
      "           182,    50,   150,    24,    85,     2,  4031,  5935,   107,    96,\n",
      "            28,  1867,   602,    19,    52,   162,    21,  1698,     8,     6,\n",
      "          1181,   367,     2,   351,    10,   140,   419,     4,   333,     5,\n",
      "          6022,  7136,  5055,  1209, 10892,    32,   219,     9,     2,   405,\n",
      "          1413,    13,  4031,    13,  1099,     7,    85,    19,     2,    20,\n",
      "          1018,     4,    85,   565,    34,    24,   807,    55,     5,    68,\n",
      "           658,    10,   507,     8,     4,   668,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   10,   121,    24,    28,    98,    74,   589,     9,   149,     2,\n",
      "          7372,  3030, 14543,  1012,   520,     2,   985,  2327,     5, 16847,\n",
      "          5479,    19,    25,    67,    76,  3478,    38,     2,  7372,     3,\n",
      "            25,    67,    76,  2951,    34,    35, 10893,   155,   449, 29495,\n",
      "         23725,    10,    67,     2,   554,    12, 14543,    67,    91,     4,\n",
      "            50,    20,    19,     8,    67,    24,  4228,     2,  2142,    37,\n",
      "            33,  3478,    87,     3,  2564,   160,   155,    11,   634,   126,\n",
      "            24,   158,    72,   286,    13,   373,     2,  4804,    19,     2,\n",
      "          7372,  6794,     6,    30,   128,    73,    48,    10,   886,     8,\n",
      "            13,    24,     4,    85,    20,    19,     8,    13,    35,   218,\n",
      "             3,   428,   710,     2,   107,   936,     7,    54,    72,   223,\n",
      "             3,    10,    96,   122,     2,   103,    54,    72,    82,     2,\n",
      "           658,   202,     2,   106,   293,   103,     7,  1193,     3,  3031,\n",
      "           708,  5760,     3,  2918,  3991,   706,  3327,   349,   148,   286,\n",
      "            13,   139,     6,     2,  1501,   750,    29,  1407,    62,    65,\n",
      "          2612,    71,    40,    14,     4,   547,     9,    62,     8,  7943,\n",
      "            71,    14,     2,  5687,     5,  4868,  3111,     6,   205,     2,\n",
      "            18,    55,  2075,     3,   403,    12,  3111,   231,    45,     5,\n",
      "           271,     3,    68,  1400,     7,  9774,   932,    10,   102,     2,\n",
      "            20,   143,    28,    76,    55,  3810,     9,  2723,     5,    12,\n",
      "            10,   379,     2,  7372,    15,     4,    50,   710,     8,    13,\n",
      "            24,   887,    32,    31,    19,     8,    13,   428],\n",
      "        [18923,     7,     4,  4753,  1669,    12,  3019,     6,     4, 13906,\n",
      "           502,    40,    25,    77,  1588,     9,   115,     6, 21713,     2,\n",
      "            90,   305,   237,     9,   502,    33,    77,   376,     4, 16848,\n",
      "           847,    62,    77,   131,     9,     2,  1580,   338,     5, 18923,\n",
      "            32,     2,  1980,    49,   157,   306, 21713,    46,   981,     6,\n",
      "         10298,     2, 18924,   125,     9,   502,     3,   453,     4,  1852,\n",
      "           630,   407,  3407,    34,   277,    29,   242,     2, 20200,     5,\n",
      "         18923,    77,    95,    41,  1833,     6,  2105,    56,     3,   495,\n",
      "           214,   528,     2,  3479,     2,   112,     7,   181,  1813,     3,\n",
      "           597,     5,     2,   156,   294,     4,   543,   173,     9,  1562,\n",
      "           289, 10038,     5,     2,    20,    26,   841,  1392,    62,   130,\n",
      "           111,    72,   832,    26,   181, 12402,    15,    69,   183,     6,\n",
      "            66,    55,   936,     5,     2,    63,     8,     7,    43,     4,\n",
      "            78, 23726, 15995,    13,    20,    17,   800,     5,   392,    59,\n",
      "          3992,     3,   371,   103,  2596,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "print(text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803f831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "#label of each sequence in the batch\n",
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa670c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([165.,  86., 218., 145.])\n"
     ]
    }
   ],
   "source": [
    "#length of each sequence in the batch\n",
    "print(length_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f1cf9ab",
   "metadata": {},
   "source": [
    "From the previous output, I can see that the longest review has 218 words. Consequently, the other examples in the mini-batch are zero-padded so that they all have the same length,so they can be stored efficiently in a tensor.\n",
    "\n",
    "In the following Python cell for instance, I print the shape of the mini-batch to illustrate this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e86223e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 218])\n"
     ]
    }
   ],
   "source": [
    "print(text_batch.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7233bb03",
   "metadata": {},
   "source": [
    "Many things have happened, at least in my opinion. We loaded the training and testing partitions of the IMDb dataset from the `torchtext.datasets` module. We then further splitted training partitions\n",
    "\n",
    "I then with tokenized the training examples, and assigned a unique integer to each token discovered using Pytorch's `vocab` object.\n",
    "\n",
    "I finish with creating a small batches of 4 examples to show that everything is working."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a7207bb",
   "metadata": {},
   "source": [
    "Finally, let's divide all three datasets: training, validation, and testing into proper data loader with batch size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d8a15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                      shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                      shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77df38a0",
   "metadata": {},
   "source": [
    "I now, have proper dataloaders now! The data is in a suitable format for the RNN. I am ready to move to finally move on something else. The next thing I want to discuss is feature **embedding**, which is optional but a highly recommended preprocessing step that is used to reduce the dimensionality of the word vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3492be7a",
   "metadata": {},
   "source": [
    "# Embedding layers for sentence encoding\n",
    "\n",
    "The previous section was dedicated to loading the dataset, and preprocess it. At this point, each sequence of words (i.e. reviews) was turned into a sequence of integers values that corresponds to indices of unique words. Then the dataset has been put into `DataLoader`s that we can iterate on, and extract batches. But are we ready to feed those integer sequences into an RNN? 🤔\n",
    "\n",
    "No. Really, No."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
