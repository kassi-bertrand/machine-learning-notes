{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b434d57",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks - Smile Classifier\n",
    "\n",
    "In this notebook, I implement a CNN to classify face images based on smiles. I also explore data augmentation techiniques to enhance the model's performance. Let's jump in right away and load the celebA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88e820",
   "metadata": {},
   "source": [
    "# Loading the CelebA dataset\n",
    "\n",
    "CelebFaces Attributes Dataset, or CelebA for short, is an image dataset that identifies celebrity face attributes. It contains 202,599 face images across five landmark locations, with 40 binary attribute annotations for each image. \n",
    "\n",
    "Tought the dataset is available through the PyTorch's `torchvision` module, the link appears to be unstable. So, I downloaded the dataset manually using this [link](https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db40d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "image_path = './dataset'\n",
    "\n",
    "#Load training partition of the dataset\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='train',\n",
    "    target_type='attr', download=False\n",
    ")\n",
    "\n",
    "#Load validation partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='valid',\n",
    "    target_type='attr', download=False\n",
    ")\n",
    "\n",
    "#Load testing partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='test',\n",
    "    target_type='attr', download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ea67f",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "**Data augmentation** refers to a set of techniques for dealing with cases where the training data is limited. Those techniques let us modify or even synthesize more data to bring more variation in the dataset which is good.\n",
    "\n",
    "To augment our dataset, we need to perform \"transformations\" on it. Remember, in the folder 03 in `mpl-torch.ipynb`, I said the following:\n",
    "\n",
    "> I import the torchvision and **transforms** modules. The second module[transform], as the name suggests, let us perform common transformations on **image** data. According to the documentation, Transforms are common image transformations available in the torchvision.transforms module.\n",
    ">\n",
    ">\n",
    "> Another interesting feature is that transform operations can be **chained** together using `Compose`.\n",
    "\n",
    "Here again, I will use the `transform` module to perform the transformations and use `Compose` to chain those transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5191960d",
   "metadata": {},
   "source": [
    "Let's start with the set of transformations to perform on the training partition of the data. Only data augmentation is only applied to the training partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop([178, 178]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493eba0",
   "metadata": {},
   "source": [
    "Let's continue with specify the set of transformation to perform on both the validation and testing partition of the dataset. Notice, I am not modifying the images themselves, just changing the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b4ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop([178, 178]),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105ca33",
   "metadata": {},
   "source": [
    "With all the transformation defined, let's *reload* the partitions of the dataset, but this time... I will apply the tranformations defined in the previous cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a308910",
   "metadata": {},
   "source": [
    "In this introduction of this notebook, I said that the dataset under consideration has 40 attributes. As proof, I print the shape of `celeba_train_dataset.attr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b243334d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([162770, 40])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba_train_dataset.attr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccadd6a",
   "metadata": {},
   "source": [
    "There are 40 columns. One for each attribute. The same applies for each partition we loaded earlier. For this model, I am interested in only one of them: The **smilling** attribute, and it is the 32nd attribute.\n",
    "\n",
    "So, I write the `get_smile` function whose job will be to extract the smilling attribute from the 40 attributes. The function is be passed as `target_transform` parameter when the dataset partitions are reloaded in the cells below. \n",
    "\n",
    "When loading a dataset the function specifed as `target_transform` is passed the attribute tensor (containing target variables), and manipulates it; which in our case, is grabbing the 32nd column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69c9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_smile = lambda attr: attr[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952d3e5",
   "metadata": {},
   "source": [
    "Okay, with `get_smile` out of the way, let's reload the partitions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40512652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload training partition of the dataset\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='train',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform_train, target_transform=get_smile #extract smiling attribute\n",
    ")\n",
    "\n",
    "#Reload validation partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='valid',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")\n",
    "\n",
    "#Reload testing partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='test',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575329a4",
   "metadata": {},
   "source": [
    "# Implementing the model in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e450e",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e0107",
   "metadata": {},
   "source": [
    "# Last words..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
