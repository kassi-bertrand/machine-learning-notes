{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b434d57",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks - Smile Classifier\n",
    "\n",
    "In this notebook, I implement a CNN to classify face images based on smiles. I also explore data augmentation techiniques to enhance the model's performance.\n",
    "\n",
    "The CNN I am to create has four convolutional layers producing 32, 64, 128, and 256 feature maps respectively. All those convolutional layers use a kernel size of $3 \\times 3$ with padding 1. The first three convolution layers are followed by max-pooling layers $P_{2 \\times 2}$. Two dropout layers are also included for regularization.\n",
    "\n",
    "Let's jump in right away and load the celebA dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da88e820",
   "metadata": {},
   "source": [
    "# Loading the CelebA dataset\n",
    "\n",
    "CelebFaces Attributes Dataset, or CelebA for short, is an image dataset that identifies celebrity face attributes. It contains 202,599 face images across five landmark locations, with 40 binary attribute annotations for each image. \n",
    "\n",
    "Tought the dataset is available through the PyTorch's `torchvision` module, the link appears to be unstable. So, I downloaded the dataset manually using this [link](https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db40d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "image_path = './dataset'\n",
    "\n",
    "#Load training partition of the dataset\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='train',\n",
    "    target_type='attr', download=False\n",
    ")\n",
    "\n",
    "#Load validation partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='valid',\n",
    "    target_type='attr', download=False\n",
    ")\n",
    "\n",
    "#Load testing partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='test',\n",
    "    target_type='attr', download=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "767ea67f",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "**Data augmentation** refers to a set of techniques for dealing with cases where the training data is limited. Those techniques let us modify or even synthesize more data to bring more variation in the dataset which is good.\n",
    "\n",
    "To augment our dataset, we need to perform \"transformations\" on it. Remember, in the folder 03 in `mpl-torch.ipynb`, I said the following:\n",
    "\n",
    "> I import the torchvision and **transforms** modules. The second module[transform], as the name suggests, let us perform common transformations on **image** data. According to the documentation, Transforms are common image transformations available in the torchvision.transforms module.\n",
    ">\n",
    ">\n",
    "> Another interesting feature is that transform operations can be **chained** together using `Compose`.\n",
    "\n",
    "Here again, I will use the `transform` module to perform the transformations and use `Compose` to chain those transformations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5191960d",
   "metadata": {},
   "source": [
    "Let's start with the set of transformations to perform on the training partition of the data.\n",
    "\n",
    "**NOTE: Data augmentation is only applied to the training partition**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop([178, 178]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7493eba0",
   "metadata": {},
   "source": [
    "Let's continue with specify the set of transformation to perform on both the validation and testing partition of the dataset. \n",
    "\n",
    "**NOTE: I am not modifying the images themselves, but just croping the images, then resize them to the desired $64 \\times 64$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b4ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop([178, 178]),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c105ca33",
   "metadata": {},
   "source": [
    "With all the transformation defined, let's *reload* the partitions of the dataset, but this time... I will apply the tranformations defined in the previous cells."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a308910",
   "metadata": {},
   "source": [
    "In this introduction of this notebook, I said that the dataset under consideration has 40 attributes for *each* training example. As proof, I print the shape of `celeba_train_dataset.attr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b243334d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([162770, 40])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba_train_dataset.attr.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ccadd6a",
   "metadata": {},
   "source": [
    "There are 40 columns. One for each attribute. The same applies for each partition we loaded earlier. For this model, I am interested in only one of them: The **smilling** attribute, and it is the 32nd attribute.\n",
    "\n",
    "So, I write the `get_smile` function whose job will be to extract the smilling attribute from the 40 attributes. The function is be passed as `target_transform` parameter when the dataset partitions are reloaded in the cells below. \n",
    "\n",
    "When loading a dataset the function specifed as `target_transform` is passed the attribute tensor (containing target variables), and manipulates it; which in our case, is grabbing the 32nd column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69c9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_smile = lambda attr: attr[31]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5952d3e5",
   "metadata": {},
   "source": [
    "Okay, with `get_smile` out of the way, let's reload the partitions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40512652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload training partition of the dataset\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='train',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform_train, target_transform=get_smile #extract smiling attribute\n",
    ")\n",
    "\n",
    "#Reload validation partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='valid',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")\n",
    "\n",
    "#Reload testing partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='test',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "575329a4",
   "metadata": {},
   "source": [
    "# Implementing the model in PyTorch\n",
    "\n",
    "I implement the model now using the `torch.nn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046253d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4527dc9",
   "metadata": {},
   "source": [
    "I proceed with adding the first convolutional layer, followed by the first `ReLU` activation layer, a max-pooling layer, and dropout layer.\n",
    "\n",
    "*Note: This first convolutional layer outputs 32 feature maps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e14774",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\n",
    "    'conv1',\n",
    "    nn.Conv2d(\n",
    "        in_channels=3, out_channels=32,\n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout1', nn.Dropout(p=0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be2f1617",
   "metadata": {},
   "source": [
    "I continue with adding the second convolutional layer, followed by the second `ReLU` activation layer, another max-pooling layer, and the second dropout layer.\n",
    "\n",
    "*Note: This second convolutional layer outputs 64 feature maps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fad3030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\n",
    "    'conv2',\n",
    "    nn.Conv2d(\n",
    "        in_channels=32, out_channels=64,\n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout2', nn.Dropout(p=0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "520ca564",
   "metadata": {},
   "source": [
    "I continue and add the third convolutional layer. I follow it with a `ReLU` activation layer and a max-pooling layer.\n",
    "\n",
    "*Note: This third convolutional layer outputs 128 feature maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc18f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\n",
    "    'conv3',\n",
    "    nn.Conv2d(\n",
    "        in_channels=64, out_channels=128,\n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add_module('relu3', nn.ReLU())\n",
    "model.add_module('pool3', nn.MaxPool2d(kernel_size=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7e790d0",
   "metadata": {},
   "source": [
    "Now, I add the fourth, and final convolutional layer to the model. As before, I follow this convolutional layer with a `ReLU` activation layer, and a max-pooling layer as well.\n",
    "\n",
    "*Note: This fourth layer outputs 256 feature maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2470b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\n",
    "    'conv4',\n",
    "    nn.Conv2d(\n",
    "        in_channels=128, out_channels=256,\n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add_module('relu4', nn.ReLU())\n",
    "model.add_module('pool4', nn.MaxPool2d(kernel_size=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae005a7c",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c7e450e",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "558e0107",
   "metadata": {},
   "source": [
    "# Last words..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
