{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b434d57",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks - Smile Classifier\n",
    "\n",
    "In this notebook, I implement a CNN to classify face images based on smiles. I also explore data augmentation techiniques to enhance the model's performance.\n",
    "\n",
    "The CNN I am to create has four convolutional layers producing 32, 64, 128, and 256 feature maps respectively. All those convolutional layers use a kernel size of $3 \\times 3$ with padding 1. The first three convolution layers are followed by max-pooling layers $P_{2 \\times 2}$. Two dropout layers are also included for regularization.\n",
    "\n",
    "Let's jump in right away and load the celebA dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da88e820",
   "metadata": {},
   "source": [
    "# Loading the CelebA dataset\n",
    "\n",
    "CelebFaces Attributes Dataset, or CelebA for short, is an image dataset that identifies celebrity face attributes. It contains 202,599 face images across five landmark locations, with 40 binary attribute annotations for each image. \n",
    "\n",
    "Tought the dataset is available through the PyTorch's `torchvision` module, the link appears to be unstable. So, I downloaded the dataset manually using this [link](https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db40d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "image_path = './dataset'\n",
    "\n",
    "#Load training partition of the dataset\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='train',\n",
    "    target_type='attr', download=False\n",
    ")\n",
    "\n",
    "#Load validation partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='valid',\n",
    "    target_type='attr', download=False\n",
    ")\n",
    "\n",
    "#Load testing partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='test',\n",
    "    target_type='attr', download=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "767ea67f",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "**Data augmentation** refers to a set of techniques for dealing with cases where the training data is limited. Those techniques let us modify or even synthesize more data to bring more variation in the dataset which is good.\n",
    "\n",
    "To augment our dataset, we need to perform \"transformations\" on it. Remember, in the folder 03 in `mpl-torch.ipynb`, I said the following:\n",
    "\n",
    "> I import the torchvision and **transforms** modules. The second module[transform], as the name suggests, let us perform common transformations on **image** data. According to the documentation, Transforms are common image transformations available in the torchvision.transforms module.\n",
    ">\n",
    ">\n",
    "> Another interesting feature is that transform operations can be **chained** together using `Compose`.\n",
    "\n",
    "Here again, I will use the `transform` module to perform the transformations and use `Compose` to chain those transformations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5191960d",
   "metadata": {},
   "source": [
    "Let's start with the set of transformations to perform on the training partition of the data.\n",
    "\n",
    "**NOTE: Data augmentation is only applied to the training partition**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop([178, 178]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7493eba0",
   "metadata": {},
   "source": [
    "Let's continue with specify the set of transformation to perform on both the validation and testing partition of the dataset. Only training examples should be augmented. \n",
    "\n",
    "**NOTE: I am not modifying the images themselves, but just croping the images, then resize them to the desired $64 \\times 64$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b4ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop([178, 178]),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c105ca33",
   "metadata": {},
   "source": [
    "With all the transformation defined, let's *reload* the partitions of the dataset, but this time... I will apply the tranformations defined in the previous cells."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a308910",
   "metadata": {},
   "source": [
    "In this introduction of this notebook, I said that the dataset under consideration has 40 attributes for *each* training example. As proof, I print the shape of `celeba_train_dataset.attr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b243334d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([162770, 40])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba_train_dataset.attr.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ccadd6a",
   "metadata": {},
   "source": [
    "There are 40 columns. One for each attribute. The same applies for each partition we loaded earlier. For this model, I am interested in only one of them: The **smilling** attribute, and it is the 32nd attribute.\n",
    "\n",
    "So, I write the `get_smile` function whose job will be to extract the smilling attribute from the 40 attributes. The function is be passed as `target_transform` parameter when the dataset partitions are reloaded in the cells below. \n",
    "\n",
    "When loading a dataset the function specifed as `target_transform` is passed the attribute tensor (containing target variables), and manipulates it; which in our case, is grabbing the 32nd column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69c9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_smile = lambda attr: attr[31]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5952d3e5",
   "metadata": {},
   "source": [
    "Okay, with `get_smile` out of the way, let's reload the partitions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40512652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload training partition of the dataset\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='train',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform_train, target_transform=get_smile #extract smiling attribute\n",
    ")\n",
    "\n",
    "#Reload validation partition of the dataset\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='valid',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")\n",
    "\n",
    "#Reload testing partition of the dataset\n",
    "celeba_test_dataset = torchvision.datasets.CelebA(\n",
    "    root=image_path, split='test',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "809c5445",
   "metadata": {},
   "source": [
    "I can now, create data loaders for the three partitions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38de57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "torch.manual_seed(1)\n",
    "\n",
    "train_dl = DataLoader(celeba_train_dataset,\n",
    "                      batch_size, shuffle=True)\n",
    "\n",
    "valid_dl = DataLoader(celeba_valid_dataset,\n",
    "                      batch_size, shuffle=False)\n",
    "\n",
    "test_dl = DataLoader(celeba_test_dataset,\n",
    "                     batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "575329a4",
   "metadata": {},
   "source": [
    "# Implementing the model in PyTorch\n",
    "\n",
    "I implement the model now using the `torch.nn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "046253d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4527dc9",
   "metadata": {},
   "source": [
    "I proceed with adding the first convolutional layer, followed by the first `ReLU` activation layer, a max-pooling layer, and dropout layer.\n",
    "\n",
    "*Note: This first convolutional layer outputs 32 feature maps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e14774",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\n",
    "    'conv1',\n",
    "    nn.Conv2d(\n",
    "        in_channels=3, out_channels=32,\n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout1', nn.Dropout(p=0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be2f1617",
   "metadata": {},
   "source": [
    "I continue with adding the second convolutional layer, followed by the second `ReLU` activation layer, another max-pooling layer, and the second dropout layer.\n",
    "\n",
    "*Note: This second convolutional layer outputs 64 feature maps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fad3030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\n",
    "    'conv2',\n",
    "    nn.Conv2d(\n",
    "        in_channels=32, out_channels=64,\n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout2', nn.Dropout(p=0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "520ca564",
   "metadata": {},
   "source": [
    "I continue and add the third convolutional layer. I follow it with a `ReLU` activation layer and a max-pooling layer.\n",
    "\n",
    "*Note: This third convolutional layer outputs 128 feature maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc18f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\n",
    "    'conv3',\n",
    "    nn.Conv2d(\n",
    "        in_channels=64, out_channels=128,\n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add_module('relu3', nn.ReLU())\n",
    "model.add_module('pool3', nn.MaxPool2d(kernel_size=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7e790d0",
   "metadata": {},
   "source": [
    "Now, I add the fourth, and final convolutional layer to the model. As before, I follow this convolutional layer with a `ReLU` activation layer, but this I follow the ReLU layer with an average pooling layer and not a max-pooling layer.\n",
    "\n",
    "*Note: This fourth layer outputs 256 feature maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2470b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\n",
    "    'conv4',\n",
    "    nn.Conv2d(\n",
    "        in_channels=128, out_channels=256,\n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add_module('relu4', nn.ReLU())\n",
    "model.add_module('pool4', nn.AvgPool2d(kernel_size=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b28d4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae005a7c",
   "metadata": {},
   "source": [
    "The outputs of the fourth convolutional layer have 256 channels, where each feature map has height and width of $8$. In other words, outputs from the last convolutional layer have the shape $[N \\times 256 \\times 8 \\times 8]$, where $N$ is the batch size.\n",
    "\n",
    "This output is then fed into the Average pooling layer which computes the average value of each feature map (i.e. channel) resulting in a single value. The output of the average pooling layer is thus of the shape $[N \\times 256 \\times 1 \\times 1]$ where $N$ is the batch size.\n",
    "\n",
    "I then flatten the output of the average pooling layer, and feeds it into a fully connected layer. After flattening the shape of the output is: $[N \\times 256]$ where $N$ is the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e03ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module('flatten', nn.Flatten())\n",
    "model.add_module('fc', nn.Linear(256, 1))\n",
    "model.add_module('sigmoid', nn.Sigmoid()) #To get a final output between 0 and 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcb7a0f8",
   "metadata": {},
   "source": [
    "I finished implementing the model. I now define the loss function, and the optimizer for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9848006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b276ff9f",
   "metadata": {},
   "source": [
    "I am ready to train the model. But before going ahead and do that, I would like to print the model, to list all the layers I added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e341e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu4): ReLU()\n",
       "  (pool4): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c7e450e",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "Like before, I would like to train this model on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d6a01c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab94fe42",
   "metadata": {},
   "source": [
    "Below, the model is moved to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43108d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85d73903",
   "metadata": {},
   "source": [
    "Just like I did in `cnn-mnist.ipynb`, I now implement a `train` function where I write logic to train the model we implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7b8a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_dl, valid_dl):\n",
    "    #Loss and accuracy PER epoch during training\n",
    "    loss_hist_train = [0] * num_epochs\n",
    "    accuracy_hist_train = [0] * num_epochs\n",
    "\n",
    "    #Loss and accuracy PER epoch during validation\n",
    "    loss_hist_valid = [0] * num_epochs\n",
    "    accuracy_hist_valid = [0] * num_epochs\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            #forward pass, then compute loss\n",
    "            pred = model(x_batch)[:, 0] #grad numbers in first column\n",
    "            loss = loss_fn(pred, y_batch.float())\n",
    "\n",
    "            #Compute gradients using backprop, update weights, then reset gradients\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #running loss for current batch. Accumulated for the entire epoch\n",
    "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
    "\n",
    "            #Correct prediction for current batch. Accumulated for the entire epoch\n",
    "            is_correct = (\n",
    "                ((pred >= 0.5).float() == y_batch).float()\n",
    "            ).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum()\n",
    "\n",
    "        #Average loss and accuracy for this epoch\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch(device)\n",
    "                pred = model(x_batch)[:, 0]\n",
    "                loss = loss_fn(pred, y_batch.float())\n",
    "                loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
    "                is_correct = (\n",
    "                    ((pred >= 0.5).float() == y_batch).float()\n",
    "                ).float()\n",
    "                accuracy_hist_valid[epoch] += is_correct.sum()\n",
    "\n",
    "        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "\n",
    "        print(f'Epoch{epoch + 1}: training accuracy: '\n",
    "              f'{accuracy_hist_train[epoch]:.4f} validation accuracy: '\n",
    "              f'{accuracy_hist_valid[epoch]:.4f}')\n",
    "\n",
    "    \n",
    "    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e295d04",
   "metadata": {},
   "source": [
    "With the `train` routine done, let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25450836",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "num_epochs = 20\n",
    "history = train(model, num_epochs=num_epochs, train_dl=train_dl, valid_dl=valid_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "558e0107",
   "metadata": {},
   "source": [
    "# Last words..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
