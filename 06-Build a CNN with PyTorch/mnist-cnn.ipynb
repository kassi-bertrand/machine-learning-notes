{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0209808",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network - MNIST\n",
    "\n",
    "In this notebook, my goal is to implement a CNN to classify handwritten digits from the MNIST dataset using PyTorch. \n",
    "\n",
    "Classifying handwritten digits is not new to me at this point ðŸ˜…. In the previous folder, I used PyTorch to implement an MLP to do the very same thing, and acheived 96% accuracy on the testing set, which I think is pretty good! ðŸ¤·\n",
    "\n",
    "So, implementing this is CNN is also a chance for me to compare the performance of two different architectures (CNN and MLP) on the very same task. In `README.md` of this folder, I introduced CNNs as a family of models that are *well-suited for image-related task*. By the end of this notebook, I will see for myself! The score to beat is: 96%\n",
    "\n",
    "Okay, let's jump right in!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60a92093",
   "metadata": {},
   "source": [
    "# Model Overview\n",
    "\n",
    "The architecture of the network that I am going to implement is shown on the following image:\n",
    "\n",
    "![Deep CNN architecture](./images/img-7.jpg)\n",
    "\n",
    "It is not new to me that training examples are usually fed to networks in batches. So, I would like to describe the journey of a batch throughout the network. For simplicity, I choose 3 as the batchsize, and assume the image is black and white (i.e. one color channel) and of size $(28 \\times 28)$.\n",
    "\n",
    "When the batch of 3 images gets fed in the network, through the input layer, **EACH** image in the batch gets convolved with 32 different filters (of size $5 \\times 5$). Convolving **EACH** image in batch with 32 kernels results in 32 features maps for **EACH** image. Another way to see this is that after an image in the batch gets convolved the 32 filters, the result is a $(28 \\times 28 \\times 32)$ matrix... 32 being the features maps. So, after all three images in the batch go through this layer, the result is a 4-D matrix containing THREE $(28 \\times 28 \\times 32)$ matrices. If you do not understand this, consider reading the section of `cnn-basics.ipynb` where I talk about convolving on 3-D matrices.\n",
    "\n",
    "Still with me? Cool! Let's proceed.\n",
    "\n",
    "The three sets of 32 feature maps we got out of the convolutional layer (in the form of a 4-D matrix containing THREE $(28 \\times 28 \\times 32)$ matrices) get fed in a subsampling layer where a max-pooling operation, $P_{2 \\times 2}$, takes place. As I learned in `cnn-basics.ipynb`, pooling decreases the resolution of feature maps. So, the 32 feature maps of **EACH** image get downsized from $(28 \\times 28)$ to $(14 \\times 14)$. After the batch goes through the subsampling layer, the result is still a 4-D matrix containing THREE $(14 \\times 14 \\times 32)$. Notice, the feature maps matrices got smaller.\n",
    "\n",
    "The three sets of 32 feature maps, now of size $(14 \\times 14)$, gets fed into another convolutional layers. ~IN PROGRESS~\n",
    "\n",
    "Here is the list of layers, and the tensor size in each of them:\n",
    "\n",
    "- `Input`: [batchsize $\\times 28 \\times 28 \\times 1 $]\n",
    "\n",
    "- `Conv_1`: [batchsize $\\times 28 \\times 28 \\times 32 $]\n",
    "\n",
    "- `Pooling_1`: [batchsize $\\times 14 \\times 14 \\times 32 $]\n",
    "\n",
    "- `Conv_2`: [batchsize $\\times 14 \\times 14 \\times 64 $]\n",
    "\n",
    "- `Pooling_2`: [batchsize $\\times 28 \\times 28 \\times 1 $]\n",
    "\n",
    "- `FC_1`: [batchsize $\\times 1024$]\n",
    "\n",
    "- `FC_2` and `softmax` layer: [batchsize $\\times 1024$]\n",
    "\n",
    "Remember, in `cnn-basics.ipynb`, I said this:\n",
    "\n",
    "> Colored images are often represented by 3-D matrices. Different Deep Learning frameworks will follow **different conventions**, but I know for sure that one dimension represents the width of the image, another represents the height of the image, and the remaining one represents the number of color channel in the image.\n",
    "\n",
    "The key term is: **convention**.\n",
    "\n",
    "See, when we read an image, the default dimension for the channels is the first dimension of the tensor array (if we add a batch dimension). This is called the **`NCHW` format**, where `N` stands for the number of images within the batch, `C` stands for channels, and `H` and `W` stand for height and width, respectively.\n",
    "\n",
    "In PyTorch, `Conv2D`, the convolutional class, assumes that inputs are in the `NCHW` format by default. TensorFlow, on the other hand, uses use NHWC format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4adaec8f",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a7f6308",
   "metadata": {},
   "source": [
    "# Implementing the model using PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b62148ef",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2422d857",
   "metadata": {},
   "source": [
    "# Last words..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
