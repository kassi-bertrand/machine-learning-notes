{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0209808",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network - MNIST\n",
    "\n",
    "In this notebook, my goal is to implement a CNN to classify handwritten digits from the MNIST dataset using PyTorch. \n",
    "\n",
    "Classifying handwritten digits is not new to me at this point ðŸ˜…. In the previous folder, I used PyTorch to implement an MLP to do the very same thing, and acheived 96% accuracy on the testing set, which I think is pretty good! ðŸ¤·\n",
    "\n",
    "So, implementing this is CNN is also a chance for me to compare the performance of two different architectures (CNN and MLP) on the very same task. In `README.md` of this folder, I introduced CNNs as a family of models that are *well-suited for image-related task*. By the end of this notebook, I will see for myself! The score to beat is: 96%\n",
    "\n",
    "Okay, let's jump right in!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60a92093",
   "metadata": {},
   "source": [
    "# Model Overview\n",
    "\n",
    "The architecture of the network that I am going to implement is shown on the following image:\n",
    "\n",
    "![Deep CNN architecture](./images/img-7.jpg)\n",
    "\n",
    "It is not new to me that training examples are usually fed to networks in batches. So, I would like to describe the journey of a batch throughout the network. For simplicity, I choose 3 as the batchsize, and assume the image is black and white (i.e. one color channel) and of size $(28 \\times 28)$.\n",
    "\n",
    "$\\longrightarrow$ When the batch of 3 images gets fed in the network, through the input layer, **EACH** image in the batch gets convolved with 32 different filters (of size $5 \\times 5$). Convolving **EACH** image in batch with 32 kernels results in **32** features maps for **EACH** image. Another way to see this is that after an image in the batch gets convolved the 32 filters, the result is a $(28 \\times 28 \\times 32)$ matrix... 32 channels (i.e. features maps). So, after all three images in the batch go through this layer, the result is a 4-D matrix containing THREE $(28 \\times 28 \\times 32)$ matrices. If you do not understand this, consider reading the section of `cnn-basics.ipynb` where I talk about convolving on 3-D matrices.\n",
    "\n",
    "Still with me? Cool! Let's proceed.\n",
    "\n",
    "$\\longrightarrow$ The three sets of 32 feature maps we got out of the convolutional layer (in the form of a 4-D matrix containing THREE $(28 \\times 28 \\times 32)$ matrices) get fed in a subsampling layer where a max-pooling operation, $P_{2 \\times 2}$, takes place. As I learned in `cnn-basics.ipynb`, pooling decreases the resolution of feature maps. So, the 32 feature maps of **EACH** image get downsized from $(28 \\times 28)$ to $(14 \\times 14)$. After the batch goes through the subsampling layer, the result is still a 4-D matrix containing THREE smaller $(14 \\times 14 \\times 32)$ feature maps matrices.\n",
    "\n",
    "$\\longrightarrow$ The batch of feature map matrices now gets fed into another convolutional layer. Each feature map matrix gets convolved with 64 kernels. Remember in `cnn-basics.ipynb`, I said the following when talking about performing convolutions:\n",
    "\n",
    ">  The filters must have the SAME number of channels as the input image\n",
    "\n",
    "Here, we are feeding 3 feature map matrices (32 channels each) in the conv. layer. So, each of the 64 filters must also have 32 channels. After convolving the 64 filters (of size $5 \\times 5$) with each feature map matrix, the result is a batch of 3 new feature map matrices of 64 channels each.\n",
    "\n",
    "Are still there?! Hang on, almost done. I need to understand it. Articulating helps!\n",
    "\n",
    "$\\longrightarrow$ Next is another pooling layer. The batch containing our THREE $(14 \\times 14 \\times 64)$ matrices get fedin a sampling layer where a max-pooling operation, $P_{2 \\times 2}$, takes place. The result our batch with three smaller $(7 \\times 7 \\times 64)$ feature maps matrices.\n",
    "\n",
    "$\\longrightarrow$ Each feature map matrix in the batch is flatten, then fed in to MLP basically, and the rest I familiar already :)\n",
    "\n",
    "Here is the list of layers I described, and the tensor size to feed them:\n",
    "\n",
    "- `Input`: [batchsize $\\times 28 \\times 28 \\times 1 $]\n",
    "\n",
    "- `Conv_1`: [batchsize $\\times 28 \\times 28 \\times 32 $]\n",
    "\n",
    "- `Pooling_1`: [batchsize $\\times 14 \\times 14 \\times 32 $]\n",
    "\n",
    "- `Conv_2`: [batchsize $\\times 14 \\times 14 \\times 64 $]\n",
    "\n",
    "- `Pooling_2`: [batchsize $\\times 28 \\times 28 \\times 1 $]\n",
    "\n",
    "- `FC_1`: [batchsize $\\times 1024$]\n",
    "\n",
    "- `FC_2` and `softmax` layer: [batchsize $\\times 10$]\n",
    "\n",
    "Remember, in `cnn-basics.ipynb`, I said this:\n",
    "\n",
    "> Colored images are often represented by 3-D matrices. Different Deep Learning frameworks will follow **different conventions**, but I know for sure that one dimension represents the width of the image, another represents the height of the image, and the remaining one represents the number of color channel in the image.\n",
    "\n",
    "The key term is: **convention**.\n",
    "\n",
    "Above, I followed my own convention to help my understanding. ðŸ¤·\n",
    "\n",
    "But, when we read an image, the default dimension for the channels is the first dimension of the tensor array (if we add a batch dimension). This is called the **`NCHW` format**, where `N` stands for the number of images within the batch, `C` stands for channels, and `H` and `W` stand for height and width, respectively.\n",
    "\n",
    "In PyTorch, `Conv2D`, the convolutional class, assumes that inputs are in the `NCHW` format by default. TensorFlow, on the other hand, uses use NHWC format. Moving forward, I will follow the `NCHW` format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4adaec8f",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing the data\n",
    "\n",
    "As I said earlier, this CNN is to classify handwritten digits from the MNIST dataset. So let's go ahead and load the dataset using `torchvision` module.\n",
    "\n",
    "The MNIST dataset come with pre-specified **training** and **testing** partitions. Before, I used the **training** partition to train, and the **testing** partition to test. This time however, I use a portion of the training partition for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504fdfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./dataset\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba490e1741d4e3fbd23e43d4c27b598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./dataset\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2617386cbc4047b4a9bf212678eee28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./dataset\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52588a649e8347d6a023d8ac66eeb54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./dataset\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886c0f91be904e9a847ab337c0f9be9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./dataset\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "image_path = './dataset'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#Download the training partition of the MNIST dataset \n",
    "mnist_dataset = torchvision.datasets.MNIST(\n",
    "    root=image_path, train=True,\n",
    "    transform=transform, download=True\n",
    ")\n",
    "\n",
    "#Split the training partition in two. and use one portion \n",
    "#for validation \n",
    "mnist_valid_dataset = Subset(\n",
    "    mnist_dataset,\n",
    "    torch.arange(10000) #Use the [0, 10000) examples for validation\n",
    ")\n",
    "\n",
    "mnist_train_dataset = Subset(\n",
    "    mnist_dataset,\n",
    "    torch.arange(10000, len(mnist_dataset)) #Use [10000, data size) for training\n",
    ")\n",
    "\n",
    "#Download the test part of the dataset\n",
    "mnist_test_dataset = torchvision.datasets.MNIST(\n",
    "    root=image_path, train=False,\n",
    "    transform=transform, download=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "457da1b0",
   "metadata": {},
   "source": [
    "Let's create some `DataLoader` instances to help us iterate on our dataset in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a287390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "\n",
    "train_dl = DataLoader(mnist_train_dataset,\n",
    "                      batch_size,\n",
    "                      shuffle=True)\n",
    "\n",
    "valid_dl = DataLoader(mnist_valid_dataset,\n",
    "                      batch_size,\n",
    "                      shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1a56d82",
   "metadata": {},
   "source": [
    "I am now ready to implement the CNN architecture described earlier. Let's do this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a7f6308",
   "metadata": {},
   "source": [
    "# Implementing the model using PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b62148ef",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2422d857",
   "metadata": {},
   "source": [
    "# Last words..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
